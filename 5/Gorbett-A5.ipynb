{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matt Gorbett"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a fun assignment.  I had a good time trying to find the best network structures possible without spending too much time.  The larger network was better for regression, but it took a long time.  I implemented my iteration count by seeing when the larger model structure started to overfit.  I'll have a look at the source code in the included files later this week to get a better feel for this neural network code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions\n",
    "\n",
    "### trainNNs\n",
    "ml.partition is a simple function to break out the arrays for training inputs/outputs and testing inputs/outputs.  This was convenient.  Next, I want to iterate through each hidden layer I input into the function, which I do with a simple for loop.  I start taking time, because we want to log how many times it takes the network to run N number of repetitions on a given structure, per the requirements.  \n",
    "Next, I have another for loop that tells us to run the neural network structure N number of times.  This will give us an idea of how the network performs generally, rather than just once.  Since training can give varying weights to a neural networks nodes, we want to gain a general idea of how this structure performs.  \n",
    "The only difference between regression and classification is which constructor you use.  This is handled in the next lines of code with either using NeuralNetworkClassifier or NeuralNetwork constructor.  \n",
    "Next I train the network with the training data and predict the test data.  I use SkLearn's mean_squared_error to obtain RMSE results.  I append these to separate lists for train and test data and continue the loops, logging time and results where applicable.  \n",
    "\n",
    "### summarize\n",
    "summarize gets an average of the N different runs of each network.  This just involves iterating through each network structures' results and taking the average RMSE result of each train and test sets predictions.  np.average worked well here.  I then added the structure and duration along with these averages into a new list and returned.  \n",
    "\n",
    "### bestNetwork\n",
    "This function finds the network structure in the summary results that has the best test RMSE.  This is determined by finding the network with the lowest error value, the third element of the summary array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neuralnetworks as nn\n",
    "import mlutils as ml \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#global errors\n",
    "#errors = []\n",
    "\n",
    "def trainNNs(X, T, trainFraction, hiddenLayerStructures, numberRepetitions, numberIterations, classify=False):\n",
    "    results=[]\n",
    "    Xtrain,Ttrain,Xtest,Ttest=ml.partition(X,T,[trainFraction, 1-trainFraction], classification=classify)\n",
    "    i=0\n",
    "    for hiddenLayer in hiddenLayerStructures:\n",
    "        i+=1\n",
    "        trainList=[]\n",
    "        testList=[]\n",
    "        now = time.time()\n",
    "        for repetition in range(0,numberRepetitions):\n",
    "            if(classify):\n",
    "                classifiers=int(T.max(axis=0))+1\n",
    "                nnet = nn.NeuralNetworkClassifier(X.shape[1], hiddenLayer, classifiers)\n",
    "            else:     \n",
    "                nnet = nn.NeuralNetwork(X.shape[1], hiddenLayer, T.shape[1])\n",
    "            \n",
    "            nnet.train(Xtrain, Ttrain, numberIterations)\n",
    "            trainPredict=nnet.use(Xtrain)\n",
    "            testPredict=nnet.use(Xtest)\n",
    "            rmseTrain = sqrt(mean_squared_error(Ttrain, trainPredict))\n",
    "            rmseTest=sqrt(mean_squared_error(Ttest, testPredict))\n",
    "            print(\"Network #\"+ str(i)+\" repetition #\"+ str(repetition+1)+ \" has test RMSE=\"+str(rmseTest))\n",
    "            trainList.append(rmseTrain)\n",
    "            testList.append(rmseTest)\n",
    "            \n",
    "        later = time.time()\n",
    "        duration=later-now\n",
    "        errors.append([hiddenLayer, nnet.getErrorTrace()])\n",
    "        results.append([hiddenLayer, trainList, testList, duration])\n",
    "    return results\n",
    "\n",
    "def summarize(results):\n",
    "    summary=[]\n",
    "    for x in results:\n",
    "        trainAvg=np.average(x[1])\n",
    "        testAvg=np.average(x[2])\n",
    "        summary.append([x[0],trainAvg,testAvg,x[3]])\n",
    "    return summary\n",
    "        \n",
    "    \n",
    "def bestNetwork(summary):\n",
    "    lowest=summary[0][2]\n",
    "    lowestX=summary[0]\n",
    "    for x in summary:\n",
    "        if(x[2]<lowest):\n",
    "            lowest=x[2]\n",
    "            lowestX=x\n",
    "    return lowestX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Experiment\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split this experiment into a few different sections:\n",
    "1.  Load data and experiment with optimal number of iterations.  \n",
    "2.  Run main experiment and print results.\n",
    "3.  Train optimal network structure with training set and predict test data.  Then graph results to see how I did.  \n",
    "4.  Plot error trace for each network\n",
    "\n",
    "\n",
    "\n",
    "To find the optimal number of iterations:\n",
    "--  Train the largest network I plan on testing with the energy data.\n",
    "--  Print error trace to see if training is still occuring.  Since error is still decreasing after 1000 iterations on train data, I want to test whether overfitting is occuring.\n",
    "--  Test for overfitting on test data by checking the RMSE on train and test data for a list of iterations.  This will determine the ideal number of iterations to train with.  \n",
    "\n",
    "Model started to overfit at 1000 iterations.  \n",
    "\n",
    "IDEAL ITERATIONS=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 100\n",
      "RMSE on train data: 62.04301580595364\n",
      "RMSE on test data: 62.3219509548606\n",
      "\n",
      "Iterations: 200\n",
      "RMSE on train data: 55.45469579836838\n",
      "RMSE on test data: 59.04164998276852\n",
      "\n",
      "Iterations: 300\n",
      "RMSE on train data: 48.68748914505726\n",
      "RMSE on test data: 56.75187265348035\n",
      "\n",
      "Iterations: 400\n",
      "RMSE on train data: 44.792834860375756\n",
      "RMSE on test data: 56.06867330155281\n",
      "\n",
      "Iterations: 500\n",
      "RMSE on train data: 40.104647380413574\n",
      "RMSE on test data: 55.23181882463125\n",
      "\n",
      "Iterations: 1000\n",
      "RMSE on train data: 35.82967082636165\n",
      "RMSE on test data: 56.70895028330729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X=pd.read_csv('energydata_complete.csv').iloc[:,1:-2]\n",
    "\n",
    "names=X.columns\n",
    "Xnames=names[2:].tolist()\n",
    "Tnames=names[:2].tolist()\n",
    "data=X.values\n",
    "Xenergy=data[:,2:]\n",
    "Tenergy=data[:,:2]\n",
    "\n",
    "\n",
    "\n",
    "#nnet.train(Xenergy, Tenergy, 1000)\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "#print(nnet.getErrorTrace())\n",
    "\n",
    "\n",
    "Xtrain,Ttrain,Xtest,Ttest=ml.partition(Xenergy,Tenergy,[.8, .2])\n",
    "\n",
    "iterationsList=[100,200,300,400,500,1000]\n",
    "for it in iterationsList:\n",
    "    nnet = nn.NeuralNetwork(Xenergy.shape[1], [50,50], Tenergy.shape[1])\n",
    "    nnet.train(Xtrain, Ttrain, it)\n",
    "    testPredict=nnet.use(Xtest)\n",
    "    rmseTest=sqrt(mean_squared_error(Ttest, testPredict))\n",
    "    trainPredict=nnet.use(Xtrain)\n",
    "    rmseTrain = sqrt(mean_squared_error(Ttrain, trainPredict))\n",
    "    print(\"Iterations: \"+str(it))\n",
    "    print(\"RMSE on train data: \" +str(rmseTrain))\n",
    "    print(\"RMSE on test data: \" +str(rmseTest))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "Below I train and test on 10 different networks 3 times for 500 iterations each.  At the bottom I graph the error trace for each models last run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining 10 neural networks 3 times each for 500 iterations\")\n",
    "results = trainNNs(Xenergy, Tenergy, .8, [0,1,10,20,50, 100, 500,[10,10], [20,20], [50,50]], 3, 500)\n",
    "\n",
    "\n",
    "summary=summarize(results)\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Best Network:\")\n",
    "best=bestNetwork(summarize(results))\n",
    "print('Hidden Layers {} Average RMSE Training {:.2f} Testing {:.2f} Took {:.2f} seconds'.format(*best))\n",
    "\n",
    "print(\"\\nError trace for each model\")\n",
    "%matplotlib inline\n",
    "for i in range(len(errors)):\n",
    "    plt.plot(errors[i][1], label=errors[i][0])\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Below I train on the best network and print out the first 50 predictions compared to their actual results for both appliances and lights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Ttrain,Xtest,Ttest=ml.partition(Xenergy,Tenergy,[.8,.2])\n",
    "nnet = nn.NeuralNetwork(Xtrain.shape[1], best[0], Ttrain.shape[1])\n",
    "\n",
    "nnet.train(Xtrain, Ttrain, 500)\n",
    "testPredict=nnet.use(Xtest)\n",
    "\n",
    "appliancesPredict = [item[0] for item in testPredict]\n",
    "lightsPredict = [item[1] for item in testPredict]\n",
    "\n",
    "appliancesActual = [item[0] for item in Ttest]\n",
    "lightsActual = [item[1] for item in Ttest]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.title(\"Appliances\")\n",
    "plt.plot(np.arange(len(appliancesPredict))[0:50], appliancesActual[0:50], 'o-', label='actual');\n",
    "plt.plot(np.arange(len(appliancesPredict))[0:50], appliancesPredict[0:50], 'o-', label='prediction');\n",
    "plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.title(\"Lights\")\n",
    "plt.plot(np.arange(len(lightsActual))[0:50], lightsActual[0:50], 'o-', label='actual');\n",
    "plt.plot(np.arange(len(lightsPredict))[0:50], lightsPredict[0:50], 'o-', label='prediction');\n",
    "plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Experiment\n",
    "\n",
    "I split the cells up as follows:\n",
    "1.  Load data, make sure the arrays are loaded properly.\n",
    "2.  Find best number of iterations like above.\n",
    "3.  Train, test, and graph the best model with a confusion matrix. \n",
    "\n",
    "Pandas has a function called \"Categorical\", which made it easy to convert each classification into an integer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.options.display.precision = 4\n",
    "Xanuran=pd.read_csv('Frogs_MFCCs.csv').iloc[:,1:22]\n",
    "Y=pd.read_csv('Frogs_MFCCs.csv').iloc[:,24]\n",
    "Y = pd.Categorical(Y)\n",
    "Tanuran = Y.codes\n",
    "Tanuran=Tanuran[:,np.newaxis]\n",
    "\n",
    "Xanuran.shape, Tanuran.shape\n",
    "Xanuran=Xanuran.values\n",
    "\n",
    "classes=np.unique(Y)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{} samples in class {}'.format(np.sum(Tanuran==i), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterations\n",
    "\n",
    "Below I do the same as above and try to get the best number of iterations to train without overfitting my biggest structure.  \n",
    "\n",
    "Iterations=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Ttrain,Xtest,Ttest=ml.partition(Xanuran,Tanuran,[.8, .2])\n",
    "\n",
    "iterationsList=[100,200,300,400,500,1000]\n",
    "for it in iterationsList:\n",
    "    nnet = nn.NeuralNetwork(Xanuran.shape[1], [50,50], Tanuran.shape[1])\n",
    "    nnet.train(Xtrain, Ttrain, it)\n",
    "    testPredict=nnet.use(Xtest)\n",
    "    rmseTest=sqrt(mean_squared_error(Ttest, testPredict))\n",
    "    trainPredict=nnet.use(Xtrain)\n",
    "    rmseTrain = sqrt(mean_squared_error(Ttrain, trainPredict))\n",
    "    print(\"Iterations: \"+str(it))\n",
    "    print(\"RMSE on train data: \" +str(rmseTrain))\n",
    "    print(\"RMSE on test data: \" +str(rmseTest))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Testing\n",
    "\n",
    "The biggest thing here was implementing the classification.  I needed to get the number of categories to send to the neural network classifier.  I used RMSE as the error function, and plotted the results using a pandas confusion matrix.  Error trace is also plotted below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors=[]\n",
    "\n",
    "#results = trainNNs(Xanuran, Tanuran, 0.8, [0, 5, [5, 5]], 5, 100, classify=True)\n",
    "results = trainNNs(Xanuran, Tanuran, 0.8,[0,1,10,20,50, 100, 500,[10,10], [20,20], [50,50]], 3, 500, classify=True)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(results)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "summary=summarize(results)\n",
    "print(summary)\n",
    "print(\"\\nBest Network:\")\n",
    "best=bestNetwork(summarize(results))\n",
    "print(best)\n",
    "\n",
    "\n",
    "\n",
    "Xtrain,Ttrain,Xtest,Ttest=ml.partition(Xanuran, Tanuran,[.8,.2])\n",
    "\n",
    "classifiers=int(Tanuran.max(axis=0))+1\n",
    "nnet = nn.NeuralNetworkClassifier(Xtrain.shape[1], best[0], classifiers)\n",
    "\n",
    "nnet.train(Xtrain,Ttrain, 500)\n",
    "testPredict=nnet.use(Xtest)\n",
    "\n",
    "\n",
    "testPredict = testPredict[:,0]\n",
    "Ttest = Ttest[:,0]\n",
    "\n",
    "#tried to use ml.confusionMatrix, but it looked kind of ugly.  Using pandas instead.  \n",
    "#ml.confusionMatrix(Ttest,testPredict, [0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "print(\"\\nConfusion Matrix of Actual/Predicted for test set of the best model\")\n",
    "y_actu = pd.Series(Ttest, name='Actual')\n",
    "y_pred = pd.Series(testPredict, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred, margins=True)\n",
    "print(df_confusion)\n",
    "\n",
    "print(\"\\nError trace for each model\")\n",
    "%matplotlib inline\n",
    "for i in range(len(errors)):\n",
    "    plt.plot(errors[i][1], label=errors[i][0])\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================= Code Execution =======================\n",
      "\n",
      "\n",
      "Testing summarize([[[1,1], [1.2, 1.3, 1.4], [2.2, 2.3, 2.4], 0.5], [[2,2,2], [4.4, 4.3, 4.2], [6.5, 6.4, 6.3], 0.6]])\n",
      "\n",
      "--- 10/10 points. Correctly returned [[[1, 1], 1.3, 2.3000000000000003, 0.5], [[2, 2, 2], 4.3, 6.3999999999999995, 0.6]]\n",
      "\n",
      "Testing bestNetwork([[[1, 1], 1.3, 2.3, 0.5], [[2, 2, 2], 4.3, 1.3, 0.6]])\n",
      "\n",
      "--- 10/10 points. Correctly returned [[2, 2, 2], 4.3, 1.3, 0.6]\n",
      "\n",
      "X = np.random.uniform(-1, 1, (100, 3))\n",
      "T = np.hstack(((X**2 - 0.2*X**3).sum(axis=1,keepdims=True),\n",
      "               (np.sin(X)).sum(axis=1,keepdims=True)))\n",
      "result = trainNNs(X, T, 0.7, [0, 5, 10, [20, 20]], 10, 100, False)\n",
      "Network #1 repetition #1 has test RMSE=0.33780054524861164\n",
      "Network #1 repetition #2 has test RMSE=0.33780054524861164\n",
      "Network #1 repetition #3 has test RMSE=0.33780054524861164\n",
      "Network #1 repetition #4 has test RMSE=0.33780054524861164\n",
      "Network #1 repetition #5 has test RMSE=0.33780054524861164\n",
      "Network #1 repetition #6 has test RMSE=0.33780054524861164\n",
      "Network #1 repetition #7 has test RMSE=0.33780054524861164\n",
      "Network #1 repetition #8 has test RMSE=0.33780054524861164\n",
      "Network #1 repetition #9 has test RMSE=0.33780054524861164\n",
      "Network #1 repetition #10 has test RMSE=0.33780054524861164\n",
      "Network #2 repetition #1 has test RMSE=0.11319308524704921\n",
      "Network #2 repetition #2 has test RMSE=0.07715576251505497\n",
      "Network #2 repetition #3 has test RMSE=0.11158235719893501\n",
      "Network #2 repetition #4 has test RMSE=0.1000220829567692\n",
      "Network #2 repetition #5 has test RMSE=0.08250491931949298\n",
      "Network #2 repetition #6 has test RMSE=0.07675666324725704\n",
      "Network #2 repetition #7 has test RMSE=0.08848383583741622\n",
      "Network #2 repetition #8 has test RMSE=0.12266779213758186\n",
      "Network #2 repetition #9 has test RMSE=0.0756504701850671\n",
      "Network #2 repetition #10 has test RMSE=0.07896471004313795\n",
      "Network #3 repetition #1 has test RMSE=0.056720412559330455\n",
      "Network #3 repetition #2 has test RMSE=0.055737059694853874\n",
      "Network #3 repetition #3 has test RMSE=0.0591696038332349\n",
      "Network #3 repetition #4 has test RMSE=0.06282453702680138\n",
      "Network #3 repetition #5 has test RMSE=0.058626928151592386\n",
      "Network #3 repetition #6 has test RMSE=0.05658590523904603\n",
      "Network #3 repetition #7 has test RMSE=0.04577664946619458\n",
      "Network #3 repetition #8 has test RMSE=0.062277025036709004\n",
      "Network #3 repetition #9 has test RMSE=0.05741699421477307\n",
      "Network #3 repetition #10 has test RMSE=0.051659716635650256\n",
      "Network #4 repetition #1 has test RMSE=0.0388009821907201\n",
      "Network #4 repetition #2 has test RMSE=0.04417230171019011\n",
      "Network #4 repetition #3 has test RMSE=0.03634246518839968\n",
      "Network #4 repetition #4 has test RMSE=0.03687549291385098\n",
      "Network #4 repetition #5 has test RMSE=0.03378614036123673\n",
      "Network #4 repetition #6 has test RMSE=0.03664985993701815\n",
      "Network #4 repetition #7 has test RMSE=0.040518099252320514\n",
      "Network #4 repetition #8 has test RMSE=0.03479654383336181\n",
      "Network #4 repetition #9 has test RMSE=0.032229842156910535\n",
      "Network #4 repetition #10 has test RMSE=0.032536590625114804\n",
      "\n",
      "--- 20/20 points. Correct.\n",
      "\n",
      "Testing bestNetwork(summarize(result))\n",
      "\n",
      "--- 20/20 points. You correctly found that network [20, 20] is best.\n",
      "\n",
      "5 Execution Grade is 60 / 60\n",
      "\n",
      "======================= The regression data set =======================\n",
      "\n",
      "--- _ / 5 points. Read the data in energydata_complete.csv into variables Xenergy and Tenergy.\n",
      "\n",
      "--- _ / 5 points. Train some networks by calling the NeuralNetwork constructor and train method and plot the error trace to help you decide now many iterations might be needed.\n",
      "\n",
      "--- _ / 5 points. Try at least 10 different hidden layer structures using trainNNs.\n",
      "\n",
      "--- _ / 5 points. Train another network with your best hidden layer structure on 0.8 of the data and test it on remaining 0.2 of the data. Plot the predicted and actual Appliances energy use, and the predicted and actual lights energy use, in two separate plots. Discuss what you see.\n",
      "\n",
      "======================= Classification data set =======================\n",
      "\n",
      "--- _ / 5 points. Read the data in Frogs_MFCCs.csv into variables Xanuran and Tanuran.\n",
      "\n",
      "--- _ / 5 points. Train some networks by calling the NeuralNetwork constructor and train method and plot the error trace to help you decide now many iterations might be needed.\n",
      "\n",
      "--- _ / 5 points. Try at least 10 different hidden layer structures using trainNNs.\n",
      "\n",
      "--- _ / 5 points. Train another network with your best hidden layer structure on 0.8 of the data and test it on remaining 0.2 of the data. Plot the predicted and actual class labels. Discuss what you see.\n",
      "\n",
      "5 Notebook Grade is   / 40\n",
      "\n",
      "5 FINAL GRADE is  / 100\n",
      "\n",
      "\n",
      "5 EXTRA CREDIT is  / 1\n"
     ]
    }
   ],
   "source": [
    "%run -i \"A5grader.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "For regression, I'm using the dataset http://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance\n",
    "\n",
    "This is a GPU performance test.  I will be using all inputs and only one output: Run1.  \n",
    "\n",
    "I will only train and test on the first 50,000 rows because I'm having issues with the data rate on jupyter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors=[]\n",
    "data=pd.read_csv('sgemm_product.csv').iloc[:50000,0:-3]\n",
    "\n",
    "\n",
    "x=data.iloc[:,0:-1].values\n",
    "y=data.iloc[:,14].values\n",
    "y=y[:,np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTraining 10 neural networks 3 times each for 500 iterations\")\n",
    "results_ = trainNNs(x, y, .8, [0,1,10,20,50, 100, 500,[10,10], [20,20], [50,50]], 5, 500)\n",
    "\n",
    "\n",
    "summary_=summarize(results_)\n",
    "print(\"\\nSummary:\")\n",
    "print(summary_)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Best Network:\")\n",
    "best_=bestNetwork(summarize(results))\n",
    "print('Hidden Layers {} Average RMSE Training {:.2f} Testing {:.2f} Took {:.2f} seconds'.format(*best))\n",
    "\n",
    "print(\"\\nError trace for each model\")\n",
    "%matplotlib inline\n",
    "for i in range(len(errors)):\n",
    "    plt.plot(errors[i][1], label=errors[i][0])\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "For classification I'm using http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra\n",
    "I'll be using all columns. \n",
    "This is a binary classification problem with a balanced dataset of cancer/non-cancer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('dataR2.csv').iloc[:,0:-1]\n",
    "y_=pd.read_csv('dataR2.csv').iloc[:,-1]\n",
    "\n",
    "\n",
    "y_ = pd.Categorical(y_)\n",
    "y = y_.codes\n",
    "y=y[:,np.newaxis]\n",
    "\n",
    "x.shape, y.shape\n",
    "x=x.values\n",
    "\n",
    "classes=np.unique(y)\n",
    "\n",
    "for i in range(2):\n",
    "    print('{} samples in class {}'.format(np.sum(y==i), i))\n",
    "    \n",
    "    \n",
    "results = trainNNs(x, y, 0.8,[0,1,10,20,50, 100, 500,[10,10], [20,20], [50,50]], 5, 2000, classify=True)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(results)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "summary=summarize(results)\n",
    "print(summary)\n",
    "print(\"\\nBest Network:\")\n",
    "best=bestNetwork(summarize(results))\n",
    "print(best)\n",
    "\n",
    "\n",
    "\n",
    "Xtrain,Ttrain,Xtest,Ttest=ml.partition(x, y,[.8,.2])\n",
    "\n",
    "classifiers=int(y.max(axis=0))+1\n",
    "nnet = nn.NeuralNetworkClassifier(Xtrain.shape[1], best[0], classifiers)\n",
    "\n",
    "nnet.train(Xtrain,Ttrain, 2000)\n",
    "testPredict=nnet.use(Xtest)\n",
    "\n",
    "\n",
    "testPredict = testPredict[:,0]\n",
    "Ttest = Ttest[:,0]\n",
    "\n",
    "#tried to use ml.confusionMatrix, but it looked kind of ugly.  Using pandas instead.  \n",
    "#ml.confusionMatrix(Ttest,testPredict, [0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "print(\"\\nConfusion Matrix of Actual/Predicted for test set of the best model\")\n",
    "y_actu = pd.Series(Ttest, name='Actual')\n",
    "y_pred = pd.Series(testPredict, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred, margins=True)\n",
    "print(df_confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals\n",
    "I used code from the link to calculate the confidence interval for each model in the regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "\n",
    "CI=[]\n",
    "for r in results_:\n",
    "    mean,low,high=mean_confidence_interval(r[2])\n",
    "    CI.append([r[0],low,mean,high])\n",
    "\n",
    "    \n",
    "df_ci = pd.DataFrame(CI,columns=['Structure','Low','Mean','High'])\n",
    "print(df_ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran each structure 5 times to get a better idea of how the model performs.  All models performed very close to each other for the regression dataset, which implies that the data is very trainable.  \n",
    "\n",
    "The first structure had the lowest deviation between its low and high.  Since every model trained to the same RMSE, it was simple.  \n",
    "\n",
    "The structure with the largest deviation between low and high was 100 hidden layers.  This still only deviated by a little over 1 RMSE.   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matt Gorbett"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This past summer I worked on a project to forecast wind direction from a wind turbine dataset I came across.  Links are here: \n",
    "https://mattgorb.github.io/wind.html <br>\n",
    "https://mattgorb.github.io/wind_multivariatelstm.html\n",
    "\n",
    "I used the following ML models:\n",
    "-  Support Vector Regression with SkLearn\n",
    "-  LSTM recurrent neural network with Keras\n",
    "-  Multivariate recurrent neural network with Keras for one turbine using wind speed, wind direction, and temperature as predictors.  \n",
    "\n",
    "\n",
    "This is a time series forecasting problem with circular output values.  I had to transform the outputs to sine and cosine before training. \n",
    "\n",
    "I propose experimenting with the following in this project: \n",
    "-  Multivariate recurrent neural network for predicting multiple wind turbines.  In my project, I used data for one wind turbine.  I am hoping to structure the dataset to use the four provided wind turbines in the dataset with their included variables wind direction, wind speed, and temperature.  I am hoping multiple sets of data for single times will improve results. \n",
    "-  Currently my dataset is 365x144 rows by 6x144 columns.  This means my input features only have 144*6 values.  I want to explore increasing the number of input features and potentially decreasing the number of rows.  \n",
    "-  Rather than decreasing the number of rows, I want to explore using data from similar seasons.  For example, rather than using the previous 365 days of data, I can try using the previous 120 days of data, and retrieve the previous 3 years data for the corresponding season.  For January 1-14 2017 predictions, I can gather training data from September 1-December 31st, November 1-January 31 2016, November 1-January 31 2015, November 1-January 31 2014.  \n",
    "-  Understand RNN more.  Start with this link: https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47\n",
    "- Further explore RNN structures, with different activation functions, embedding nodes.  \n",
    "\n",
    "\n",
    "\n",
    "*** I will not be running this in a jupyter notebook.  I will be using the Ubuntu Deep Learning machine image on AWS for simple GPU setup. I ran the p2.xlarge instance, which has 61 GiB RAM, 4 CPUs, and a NVIDIA K80 GPU.  I will be recording results and graphs in the project.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have several ideas and experiments I want to try out on this dataset to see if I can improve both the prediction error and the efficiency. First I will do the first two things:\n",
    "\n",
    "1.  Increase test set size to determine error on more data.  I will increase the test set size from one day to two weeks.  This will give me a feel for how the model is performing on bigger test sets rather than just a single day.  \n",
    "\n",
    "2.  Concatenate two datasets into one: <br>\n",
    "https://opendata-renewables.engie.com/explore/dataset/la-haute-borne-data-2013-2016/ <br>\n",
    "https://opendata-renewables.engie.com/explore/dataset/la-haute-borne-data-2017-2020/table/\n",
    "Currently I'm training on only one dataset, 2013-2016.  I want to concatenate more data for better forecasting.  \n",
    "This will allow me to experiment with different structures of data for training.  \n",
    "\n",
    "# Experiments \n",
    "\n",
    "## 1.  Training data setup \n",
    "\n",
    "This is an example of how the data is setup for each model.  t=time.  For each row, I will take the sine and cosine of the wind direction, and fill its inputs with its historical values, in order.  \n",
    " \n",
    " #### SVR and single variate LSTM data setup\n",
    " sin=sine(windDirection)\n",
    " cos=cosine(windDirection)\n",
    " \n",
    "|x1    |x2    |x4    |x4    |x5    |y   |\n",
    "|------|------|------|------|------|----|\n",
    "|sin(t-5)|sin(t-4)|sin(t-3)|sin(t-2)|sin(t-1)|sin(t)|\n",
    "\n",
    "|x1    |x2    |x4    |x4    |x5    |y   |\n",
    "|------|------|------|------|------|----|\n",
    "|cos(t-5)|cos(t-4)|cos(t-3)|cos(t-2)|cos(t-1)|cos(t)|\n",
    "\n",
    " #### Multivariate LSTM data setup\n",
    " tp=temperature\n",
    " sp=wind speed\n",
    " \n",
    "|x1    |x2    |x4    |x4    |x5    |y   |\n",
    "|------|------|------|------|------|----|\n",
    "|sin(t-5)|sin(t-4)|sin(t-3)|sin(t-2)|sin(t-1)|sin(t)|\n",
    "|tp(t-5)|tp(t-4)|tp(t-3)|tp(t-2)|tp(t-1)||\n",
    "|sp(t-5)|sp(t-4)|sp(t-3)|sp(t-2)|sp(t-1)||\n",
    "\n",
    "|x1    |x2    |x4    |x4    |x5    |y   |\n",
    "|------|------|------|------|------|----|\n",
    "|cos(t-5)|cos(t-4)|cos(t-3)|cos(t-2)|cos(t-1)|cos(t)|\n",
    "|tp(t-5)|tp(t-4)|tp(t-3)|tp(t-2)|tp(t-1)||\n",
    "|sp(t-5)|sp(t-4)|sp(t-3)|sp(t-2)|sp(t-1)||\n",
    "\n",
    "Dimensions:\n",
    "Input=[rows, time_back, 3] <br>\n",
    "Output[rows,1]\n",
    "\n",
    "## Proposed multi turbine setup for project\n",
    "#### Hypothesis:\n",
    "Having more data from other wind turbines at the same time frame will help predict the wind direction of a single turbine.<br>\n",
    "In the dataset we have data for 4 wind turbines.  Currently I'm only using one.  For this model, I want to train all 4 turbines wind direction forecasts at once.  \n",
    "\n",
    "  \n",
    "\n",
    "To do this, I want to try two different structures: \n",
    "#### 1. Input=[rows, time_back, 9] <br> Output[rows,4 columns]\n",
    "<br>\n",
    "This structure has four output values for a single row.  Each row has a dimension of [time_back, 9]\n",
    "\n",
    "|x1    |x2    |x4    |x4    |x5    |y   |\n",
    "|------|------|------|------|------|----|\n",
    "|wt1_sin(t-5)|wt1_sin(t-4)|wt1_sin(t-3)|wt1_sin(t-2)|wt1_sin(t-1)|wt1_sin(t),wt2_sin(t),wt3_sin(t),wt4_sin(t)|\n",
    "|wt1_tp(t-5)|wt1_tp(t-4)|wt1_tp(t-3)|wt1_tp(t-2)|wt1_tp(t-1)||\n",
    "|wt1_sp(t-5)|wt1_sp(t-4)|wt1_sp(t-3)|wt1_sp(t-2)|wt1_sp(t-1)||\n",
    "|wt2_sin(t-5)|wt2_sin(t-4)|wt2_sin(t-3)|wt2_sin(t-2)|wt2_sin(t-1)||\n",
    "|wt2_tp(t-5)|wt2_tp(t-4)|wt2_tp(t-3)|wt2_tp(t-2)|wt2_tp(t-1)||\n",
    "|wt2_sp(t-5)|wt2_sp(t-4)|wt2_sp(t-3)|wt2_sp(t-2)|wt2_sp(t-1)||\n",
    "|wt3_sin(t-5)|wt3_sin(t-4)|wt3_sin(t-3)|wt3_sin(t-2)|wt3_sin(t-1)||\n",
    "|wt3_tp(t-5)|wt3_tp(t-4)|wt3_tp(t-3)|wt3_tp(t-2)|wt3_tp(t-1)||\n",
    "|wt3_sp(t-5)|wt3_sp(t-4)|wt3_sp(t-3)|wt3_sp(t-2)|wt3_sp(t-1)||\n",
    "|wt4_sin(t-5)|wt4_sin(t-4)|wt4_sin(t-3)|wt4_sin(t-2)|wt4_sin(t-1)||\n",
    "|wt4_tp(t-5)|wt4_tp(t-4)|wt4_tp(t-3)|wt4_tp(t-2)|wt4_tp(t-1)||\n",
    "|wt4_sp(t-5)|wt4_sp(t-4)|wt4_sp(t-3)|wt4_sp(t-2)|wt4_sp(t-1)||\n",
    "\n",
    "|x1    |x2    |x4    |x4    |x5    |y   |\n",
    "|------|------|------|------|------|----|\n",
    "|wt1_cos(t-5)|wt1_cos(t-4)|wt1_cos(t-3)|wt1_cos(t-2)|wt1_cos(t-1)|wt1_cos(t),wt2_cos(t),wt3_cos(t),wt4_cos(t)|\n",
    "|wt1_tp(t-5)|wt1_tp(t-4)|wt1_tp(t-3)|wt1_tp(t-2)|wt1_tp(t-1)||\n",
    "|wt1_sp(t-5)|wt1_sp(t-4)|wt1_sp(t-3)|wt1_sp(t-2)|wt1_sp(t-1)||\n",
    "|wt2_cos(t-5)|wt2_cos(t-4)|wt2_cos(t-3)|wt2_cos(t-2)|wt2_cos(t-1)||\n",
    "|wt2_tp(t-5)|wt2_tp(t-4)|wt2_tp(t-3)|wt2_tp(t-2)|wt2_tp(t-1)||\n",
    "|wt2_sp(t-5)|wt2_sp(t-4)|wt2_sp(t-3)|wt2_sp(t-2)|wt2_sp(t-1)||\n",
    "|wt3_cos(t-5)|wt3_cos(t-4)|wt3_cos(t-3)|wt3_cos(t-2)|wt3_cos(t-1)||\n",
    "|wt3_tp(t-5)|wt3_tp(t-4)|wt3_tp(t-3)|wt3_tp(t-2)|wt3_tp(t-1)||\n",
    "|wt3_sp(t-5)|wt3_sp(t-4)|wt3_sp(t-3)|wt3_sp(t-2)|wt3_sp(t-1)||\n",
    "|wt4_cos(t-5)|wt4_cos(t-4)|wt4_cos(t-3)|wt4_cos(t-2)|wt4_cos(t-1)||\n",
    "|wt4_tp(t-5)|wt4_tp(t-4)|wt4_tp(t-3)|wt4_tp(t-2)|wt4_tp(t-1)||\n",
    "|wt4_sp(t-5)|wt4_sp(t-4)|wt4_sp(t-3)|wt4_sp(t-2)|wt4_sp(t-1)||\n",
    "\n",
    "\n",
    "#### 2. Input=[rows, time_back, 3] <br> Output[rows,1]\n",
    "<br>\n",
    "This data structure stacks the wind turbines on top of each other, keeping the same time frames in groups of four.  \n",
    "\n",
    "|x1    |x2    |x4    |x4    |x5    |y   |\n",
    "|------|------|------|------|------|----|\n",
    "|wt1_sin(t-5)|wt1_sin(t-4)|wt1_sin(t-3)|wt1_sin(t-2)|wt1_sin(t-1)|wt1_sin(t)|\n",
    "|wt1_tp(t-5)|wt1_tp(t-4)|wt1_tp(t-3)|wt1_tp(t-2)|wt1_tp(t-1)||\n",
    "|wt1_sp(t-5)|wt1_sp(t-4)|wt1_sp(t-3)|wt1_sp(t-2)|wt1_sp(t-1)||\n",
    "|wt2_sin(t-5)|wt2_sin(t-4)|wt2_sin(t-3)|wt2_sin(t-2)|wt2_sin(t-1)|wt2_sin(t)|\n",
    "|wt2_tp(t-5)|wt2_tp(t-4)|wt2_tp(t-3)|wt2_tp(t-2)|wt2_tp(t-1)||\n",
    "|wt2_sp(t-5)|wt2_sp(t-4)|wt2_sp(t-3)|wt2_sp(t-2)|wt2_sp(t-1)||\n",
    "|wt3_sin(t-5)|wt3_sin(t-4)|wt3_sin(t-3)|wt3_sin(t-2)|wt3_sin(t-1)|wt3_sin(t)|\n",
    "|wt3_tp(t-5)|wt3_tp(t-4)|wt3_tp(t-3)|wt3_tp(t-2)|wt3_tp(t-1)||\n",
    "|wt3_sp(t-5)|wt3_sp(t-4)|wt3_sp(t-3)|wt3_sp(t-2)|wt3_sp(t-1)||\n",
    "|wt4_sin(t-5)|wt4_sin(t-4)|wt4_sin(t-3)|wt4_sin(t-2)|wt4_sin(t-1)|wt4_sin(t)|\n",
    "|wt4_tp(t-5)|wt4_tp(t-4)|wt4_tp(t-3)|wt4_tp(t-2)|wt4_tp(t-1)||\n",
    "|wt4_sp(t-5)|wt4_sp(t-4)|wt4_sp(t-3)|wt4_sp(t-2)|wt4_sp(t-1)||\n",
    "\n",
    "|x1    |x2    |x4    |x4    |x5    |y   |\n",
    "|------|------|------|------|------|----|\n",
    "|wt1_cos(t-5)|wt1_cos(t-4)|wt1_cos(t-3)|wt1_cos(t-2)|wt1_cos(t-1)|wt1_cos(t)|\n",
    "|wt1_tp(t-5)|wt1_tp(t-4)|wt1_tp(t-3)|wt1_tp(t-2)|wt1_tp(t-1)||\n",
    "|wt1_sp(t-5)|wt1_sp(t-4)|wt1_sp(t-3)|wt1_sp(t-2)|wt1_sp(t-1)||\n",
    "|wt2_cos(t-5)|wt2_cos(t-4)|wt2_cos(t-3)|wt2_cos(t-2)|wt2_cos(t-1)|wt2_cos(t)|\n",
    "|wt2_tp(t-5)|wt2_tp(t-4)|wt2_tp(t-3)|wt2_tp(t-2)|wt2_tp(t-1)||\n",
    "|wt2_sp(t-5)|wt2_sp(t-4)|wt2_sp(t-3)|wt2_sp(t-2)|wt2_sp(t-1)||\n",
    "|wt3_cos(t-5)|wt3_cos(t-4)|wt3_cos(t-3)|wt3_cos(t-2)|wt3_cos(t-1)|wt3_cos(t)|\n",
    "|wt3_tp(t-5)|wt3_tp(t-4)|wt3_tp(t-3)|wt3_tp(t-2)|wt3_tp(t-1)||\n",
    "|wt3_sp(t-5)|wt3_sp(t-4)|wt3_sp(t-3)|wt3_sp(t-2)|wt3_sp(t-1)||\n",
    "|wt4_cos(t-5)|wt4_cos(t-4)|wt4_cos(t-3)|wt4_cos(t-2)|wt4_cos(t-1)|wt4_cos(t)|\n",
    "|wt4_tp(t-5)|wt4_tp(t-4)|wt4_tp(t-3)|wt4_tp(t-2)|wt4_tp(t-1)||\n",
    "|wt4_sp(t-5)|wt4_sp(t-4)|wt4_sp(t-3)|wt4_sp(t-2)|wt4_sp(t-1)||\n",
    "\n",
    "\n",
    "Dimensions:\n",
    "Input=[rows, time_back, 9] <br>\n",
    "Output[rows,4 columns for each turbine]\n",
    "\n",
    "\n",
    "\n",
    "## 2.  Alternate training data structures\n",
    "\n",
    "Once finding the ideal input data structure from above, I will do further experiments on the input data to determine whether differing row structures help with the training.  \n",
    "\n",
    "Currently I train with the following: \n",
    "1.  Training data [x,y,z] is:<br>\n",
    "    365 days (rows), 6 previous days (columns), 3 input vars (wind direction, speed, temperature)\n",
    "    \n",
    "I will experiment with the following structures:\n",
    "1.  120 days (rows), 30 previous days (columns), 3 input vars\n",
    "2.  90 days (rows), 60 previous days (columns), 3 input vars\n",
    "3.  180 days (90 from previous 90 days, 30 from test year-1, etc.), 30 previous days (columns), 3 input vars. \n",
    "\n",
    "\n",
    "## 3. LSTM Network Understanding\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "Explore alternate model structures.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Results\n",
    "\n",
    "I will run single and multivariate LSTM on larger dataset to gain a baseline RMSE and MAE.  I will compare these results to the new experiments I run.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "1.  Increase the test set size and determine performance of baseline models.  \n",
    "2.  Run experiments above and compare results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information from my previous work\n",
    "\n",
    "These are functions from my previous work that I will reuse and alter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict():\n",
    "        model = Sequential()\n",
    "        model.add(CuDNNLSTM(128*trainX_initial.shape[2]*2, input_shape=(recordsBack,trainX_initial.shape[2])))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "\n",
    "        checkpointer=ModelCheckpoint('weights.h5', monitor='val_loss', verbose=2, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "        earlystopper=EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "        model.fit(trainX_initial, trainY_initial, validation_data=(validationX, validationY),epochs=20, batch_size=testX.shape[0], verbose=2, shuffle=False,callbacks=[checkpointer, earlystopper])\n",
    "        \n",
    "        model.load_weights(\"weights.h5\")\n",
    "\n",
    "        validationPredict=model.predict(validationX)\n",
    "        validation_mae=mean_absolute_error(validationY, validationPredict)\n",
    "        \n",
    "        model.fit(trainX_initial, trainY_initial, validation_data=(validationX, validationY),epochs=1, batch_size=testX.shape[0], verbose=2)\n",
    "\n",
    "\n",
    "        testPredict = model.predict(testX)\n",
    "\n",
    "        testPredict[testPredict > 1] = 1\n",
    "        testPredict[testPredict <-1] = -1\n",
    "        return testPredict, validation_mae\n",
    "\n",
    "\n",
    "\n",
    "def convertToDegrees(sin_prediction,cos_prediction):\n",
    "\t'''\n",
    "\tConverting sine and cosine back to its circular angle depends on finding which of the the 4 circular quadrants the \n",
    "\tprediction will fall into. If sin and cos are both GT 0, degrees will fall in 0-90.  If sin>0 cos<0, degrees will fall into 90-180, etc. \n",
    "\t'''\n",
    "\tinverseSin=np.degrees(np.arcsin(sin_prediction))\n",
    "\tinverseCos=np.degrees(np.arccos(cos_prediction))\n",
    "\tradians_sin=[]\n",
    "\tradians_cos=[]\n",
    "\tfor a,b,c,d in zip(sin_prediction, cos_prediction, inverseSin, inverseCos):\n",
    "\t\tif(a>0 and b>0):\n",
    "\t\t\tradians_sin.append(c)\n",
    "\t\t\tradians_cos.append(d)\t\n",
    "\t\telif(a>0 and b<0):\n",
    "\t\t\tradians_sin.append(180-c)\n",
    "\t\t\tradians_cos.append(d)\t\n",
    "\t\telif(a<0 and b<0):\n",
    "\t\t\tradians_sin.append(180-c)\n",
    "\t\t\tradians_cos.append(360-d)\t\n",
    "\t\telif(a<0 and b>0):\n",
    "\t\t\tradians_sin.append(360+c)\n",
    "\t\t\tradians_cos.append(360-d)\n",
    "\tradians_sin=np.array(radians_sin)\n",
    "\tradians_cos=np.array(radians_cos)\n",
    "\treturn radians_sin, radians_cos\n",
    "\n",
    "\n",
    "\n",
    "def calcWeightedDegreePredictions(sin_error,cos_error,radians_sin,radians_cos):\n",
    "\terrorTotal=cos_error+sin_error\n",
    "\tsinWeight=(errorTotal-sin_error)/errorTotal\n",
    "\tcosWeight=(errorTotal-cos_error)/errorTotal\n",
    "\tweighted=np.add(sinWeight*radians_sin, cosWeight*radians_cos)\n",
    "\treturn weighted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

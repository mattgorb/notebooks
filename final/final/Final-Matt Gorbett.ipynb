{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting for Circular Data, LSTMs in NumPy, and Multi-armed Bandits\n",
    "## Matt Gorbett\n",
    "\n",
    "This project took several different paths from my project proposal. I tried to cover the various experiments from my proposal, but some proved to be a waste of time.  One part of my project proposal was to train and predict data on all 4 wind turbines rather than a single turbine.  Quick training on all 4 wind turbines proved to have bad results.  Instead, I tried different training methods such as ensembling with weights based on validation datasets, different weight initializations, and alternative validation testing approaches.  <br>\n",
    "Additionally, part of my project proposal was to understand the inner workings of an LSTM network.  I did this by building out an LSTM using only NumPy. \n",
    "Finally, the last part of my project completely diverged from my proposal.  I was listening to a podcast-This Week in Machine Learning & AI (TWiML), where a graduate student was talking about his recent paper regarding thompson sampling in deep queue networks.  This gave me the desire to implement thomspon sampling in a simple reinforcement learning problem.  I decided to implement the multi armed bandit algorithm from scratch, using a few different methods for bernoulli sampling including thompson sampling and UCB.  \n",
    "\n",
    "# Contents\n",
    "Part 1:\n",
    "1.  [Overview of previous work](#overview)\n",
    "2.  [Stationarity Analysis](#stationarity)\n",
    "3.  [Persistance Model](#persistance)\n",
    "4.  [Comparing Keras Models](#comparisons)\n",
    "5.  [Manual Ensembling: weighted with validation set MAE/RMSE](#ensembling)\n",
    "6.  [Better Validation Testing](#validation)\n",
    "7.  [Weight Initialization Testing](#weights)\n",
    "8.  [Conclusions and Future Considerations](#conclusions)\n",
    "\n",
    "Part 2: <br>\n",
    "1.  [An LSTM written in NumPy](#numpylstm)\n",
    "2.  [LSTM Code Walkthrough](#numpylstm_walkthrough)\n",
    "\n",
    "Part 3:\n",
    "1.  [Solving the multiarmed bandit problem with random, greedy, epsilon, epsilon greedy, thompson sampling, UCB policies](#rl)\n",
    "\n",
    "\n",
    "[References](#references)<br>\n",
    "[Word Count](#wordcount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overview'></a>\n",
    "# Wind Direction Forecasting-Overview\n",
    "\n",
    "Before this course I did some work predicting wind direction in a wind turbine dataset using time series forecasting.  Predicting time series can be done using LSTM, CNN, SVR, and numbers of statistical methods such as arima.  For this project I will concentrate on using LSTM networks for time series forecasting.\n",
    "\n",
    "Wind direction is a circular variable, meaning it can be from 0-360 degrees.  Predicting the direction can be done by converting the wind direction value to sine(value/360x2pi) and cosine (value/360x2pi).  These values are between -1 and 1.  I convert them back using arcsin and arccos.  \n",
    "\n",
    "Previous work from my github page with more info on my data setup:\n",
    "-  https://mattgorb.github.io/wind_multivariatelstm\n",
    "-  https://mattgorb.github.io/wind\n",
    "\n",
    "The second link has my method for converting the wind direction to sine and cosine waves and converting them back to a circular prediction.  \n",
    "\n",
    "Dataset: https://opendata-renewables.engie.com/explore/dataset/la-haute-borne-data-2013-2016/table/<br>\n",
    "Variables: wa_c_avg (wind direction), Ws (wind speed), Ot (temperature)<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stationarity'></a>\n",
    "## Stationarity Analysis\n",
    "\n",
    "First I want to determine if my data is stationary or non stationary, and whether seasonality effects the outcome.  To do this I will print out the wind direction mean and variance for each year and then each season.  \n",
    "\n",
    "A stationary time series is one whose statistical properties such as the mean, variance and autocorrelation are all constant over time. Hence, a non-stationary series is one whose statistical properties change over time. (https://www.quora.com/What-is-Stationary-series-and-non-Stationary-series)\n",
    "\n",
    "For this exercise I will use a single turbine from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turbine name: R80736\n",
      "Year: 2013 mean: 171.2583191240095 variance: 10241.806036309075\n",
      "Year: 2014 mean: 177.0322800418058 variance: 8191.131423097059\n",
      "Year: 2015 mean: 180.32319156920616 variance: 9260.0192172841\n",
      "Year: 2016 mean: 179.02708351059488 variance: 9120.598211332162\n",
      "\n",
      "Seasonality\n",
      "Winter 2013 mean: 169.06502443265148 variance: 10223.240640872438\n",
      "Winter 2014 mean: 180.0378452078669 variance: 3957.4436134315383\n",
      "Winter 2015 mean: 174.95637994976562 variance: 10118.279883664827\n",
      "Winter 2016 mean: 174.94510128351519 variance: 7648.6769787195235\n",
      "\n",
      "Spring 2013 mean: 171.466366625207 variance: 11312.092222900661\n",
      "Spring 2014 mean: 175.299488347244 variance: 11380.52232791995\n",
      "Spring 2015 mean: 182.3834821427918 variance: 10387.530680221793\n",
      "Spring 2016 mean: 195.9735849251427 variance: 8725.832637505944\n",
      "\n",
      "Summer 2013 mean: 173.70260205401698 variance: 11591.478712740089\n",
      "Summer 2014 mean: 176.94886539224964 variance: 9932.212941357642\n",
      "Summer 2015 mean: 191.19453071081915 variance: 9717.775438121686\n",
      "Summer 2016 mean: 204.16241855985962 variance: 8835.779722748583\n",
      "\n",
      "Fall 2013 mean: 168.19225210535146 variance: 8863.650662457956\n",
      "Fall 2014 mean: 173.1773510741569 variance: 6278.243815171321\n",
      "Fall 2015 mean: 174.47151649282605 variance: 7054.398742119583\n",
      "Fall 2016 mean: 144.27937186276043 variance: 9025.223219457976\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def firstTurbineData():\n",
    "\tdf = pd.read_csv('la-haute-borne-data-2013-2016.csv', sep=';')\n",
    "\tdf['Date_time'] = df['Date_time'].astype(str).str[:-6] #remove timezone (caused me an hour of pain)\n",
    "\tdf.Date_time=pd.to_datetime(df['Date_time'])\n",
    "\tdf=df.fillna(method='ffill')\n",
    "\n",
    "\tdf=df.sort_values(by='Date_time')\n",
    "\tdf = df.reset_index()\n",
    "\tturbines=df.Wind_turbine_name.unique()\n",
    "\tprint(\"Turbine name: \"+str(turbines[0]))\n",
    "\tturbineData=df[df['Wind_turbine_name']==turbines[0]]\n",
    "\treturn turbineData\n",
    "\n",
    "\n",
    "turbineData=firstTurbineData()\n",
    "\n",
    "years=turbineData.Date_time.dt.year.unique().tolist()#unique years in dataset\n",
    "years.pop(0) #remove 2012 from list since we only have the tail end of 2012 data.  \n",
    "\n",
    "#Print Yearly mean/variance for wind direction\n",
    "for y in years:\n",
    "    data=turbineData[turbineData['Date_time'].dt.year==y].Wa_c_avg.values\n",
    "    print(\"Year: \"+str(y)+ \" mean: \" + str(np.mean(data))+\" variance: \"+str(np.var(data)))\n",
    "\n",
    "\n",
    "def seasonal_statistics(data, name):\n",
    "    print(str(name)+\" mean: \" + str(np.mean(data))+\" variance: \"+str(np.var(data)))\n",
    "    \n",
    "#Seasonality is defined as 12/21-3/21, 3/22-6/21...\n",
    "print(\"\\nSeasonality\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2012, 12, 21)) & (turbineData['Date_time']<datetime.datetime(2013, 3, 21) )].Wa_c_avg.values, \"Winter 2013\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2013, 12, 21)) & (turbineData['Date_time']<datetime.datetime(2014, 3, 21) )].Wa_c_avg.values, \"Winter 2014\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2014, 12, 21)) & (turbineData['Date_time']<datetime.datetime(2015, 3, 21) )].Wa_c_avg.values, \"Winter 2015\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2015, 12, 21)) & (turbineData['Date_time']<datetime.datetime(2016, 3, 21) )].Wa_c_avg.values, \"Winter 2016\")\n",
    "\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2013, 3, 21)) & (turbineData['Date_time']<datetime.datetime(2013, 6, 21) )].Wa_c_avg.values, \"\\nSpring 2013\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2014, 3, 21)) & (turbineData['Date_time']<datetime.datetime(2014, 6, 21) )].Wa_c_avg.values, \"Spring 2014\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2015, 3, 21)) & (turbineData['Date_time']<datetime.datetime(2015, 6, 21) )].Wa_c_avg.values, \"Spring 2015\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2016, 3, 21)) & (turbineData['Date_time']<datetime.datetime(2016, 6, 21) )].Wa_c_avg.values, \"Spring 2016\")\n",
    "\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2013, 6, 21)) & (turbineData['Date_time']<datetime.datetime(2013, 9, 21) )].Wa_c_avg.values, \"\\nSummer 2013\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2014, 6, 21)) & (turbineData['Date_time']<datetime.datetime(2014, 9, 21) )].Wa_c_avg.values, \"Summer 2014\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2015, 6, 21)) & (turbineData['Date_time']<datetime.datetime(2015, 9, 21) )].Wa_c_avg.values, \"Summer 2015\" )\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2016, 6, 21)) & (turbineData['Date_time']<datetime.datetime(2016, 9, 21) )].Wa_c_avg.values, \"Summer 2016\")\n",
    "\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2013, 9, 21)) & (turbineData['Date_time']<datetime.datetime(2013, 12, 21) )].Wa_c_avg.values, \"\\nFall 2013\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2014, 9, 21)) & (turbineData['Date_time']<datetime.datetime(2014, 12, 21) )].Wa_c_avg.values, \"Fall 2014\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2015, 9, 21)) & (turbineData['Date_time']<datetime.datetime(2015, 12, 21) )].Wa_c_avg.values, \"Fall 2015\")\n",
    "seasonal_statistics(turbineData[(turbineData['Date_time']>datetime.datetime(2016, 9, 21)) & (turbineData['Date_time']<datetime.datetime(2016, 12, 21) )].Wa_c_avg.values, \"Fall 2016\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and variance stay fairly consistent over time.  There are some exceptions of course, but overall, the data has some stationarity to it.  This indicates to me that the wind is usually coming from the same direction to the wind turbine.  I would consider this data at least semi-stationary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='persistance'></a>\n",
    "## Persistance Model\n",
    "\n",
    "\n",
    "\n",
    "In the following code I calculate the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE) for the days 1/1/16 to 1/7/16 using the persistance model.  The persistance model is a basic structure that takes the time series' previous value as the prediction.  For example if you are looking to predict a value at time t then you would predict its value to be t-1.  Value t+1 would be predicted as t, and so on.  \n",
    "\n",
    "Dr. Anderson mentioned in my project proposal that I should compare any time series model with the persistance model.  I found [this link](https://towardsdatascience.com/how-not-to-use-machine-learning-for-time-series-forecasting-avoiding-the-pitfalls-19f9d7adf424), which indicates that \"Baseline forecasts with the persistence model quickly indicate whether you can do significantly better. If you can’t, you’re probably dealing with a random walk (or close to it).\"  A random walk is a stochastic process, as the name indicates.\n",
    "\n",
    "Here is the persistance model rmse and mae for a single turbine on the first six days of the year: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turbine name: R80736\n",
      "    test_date      rmse       mae\n",
      "0  2016-01-01  9.113613  6.214375\n",
      "1  2016-01-02  4.974602  3.309235\n",
      "2  2016-01-03  8.133943  5.054027\n",
      "3  2016-01-04  5.673105  3.596944\n",
      "4  2016-01-05  5.667746  4.092013\n",
      "5  2016-01-06  9.591719  5.901806\n",
      "6  2016-01-07  5.978294  3.792083\n",
      "7  2016-01-08  9.549897  7.279861\n",
      "8  2016-01-09  6.427645  4.233056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "turbineData=firstTurbineData()\n",
    "results = pd.DataFrame(columns=['test_date','rmse','mae'])\n",
    "for i in range(1,10):\n",
    "    date_to_test=datetime.datetime(2016, 1, i)\n",
    "    start_date=date_to_test-datetime.timedelta(minutes = 10)\n",
    "    end_date=date_to_test+datetime.timedelta(days = 1)\n",
    "    \n",
    "    actual=[]\n",
    "    predicted=[]\n",
    "\n",
    "    turbine=turbineData[(turbineData['Date_time']>= start_date) & (turbineData['Date_time']<end_date)].Wa_c_avg.values\n",
    "\n",
    "    actual.extend(turbine[1:])\n",
    "    predicted.extend(turbine[:-1])\n",
    "\n",
    "    mae=mean_absolute_error(actual, predicted)\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    predicted=pd.DataFrame(predicted)\n",
    "\n",
    "    results = results.append({'test_date':str(date_to_test)[:10], 'rmse': rmse,'mae': mae}, ignore_index=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the results from my Multi Variate LSTM training from a few months ago (located at https://mattgorb.github.io/wind_multivariatelstm): \n",
    "\n",
    "|Date  |RMSE|MAE|\n",
    "|------|------|------|\n",
    "|2016-01-01|11.2|7.29|\n",
    "|2016-01-02|4.5|3.28|\n",
    "|2016-01-03|5.99|4.29|\n",
    "|2016-01-04|6.40|4.08|\n",
    "|2016-01-05|5.40|3.82|\n",
    "|2016-01-06|5.78|4.01|\n",
    "|2016-01-07|7.69|5.10| \n",
    "|2016-01-08|11.69|8.2|\n",
    "|2016-01-09|7.11|4.99|\n",
    "\n",
    "#### Out of the 18 scores available here, the persistance model did better on 10/18 scores.  This indicates to me that the model is a random walk with an inability to learn from the data.  \n",
    "I was able to stack the predictions from my multivariate LSTM and the persistance model and average the predictions together, which gave me slightly before results. \n",
    "#### From here on I will be doing experiments on a smaller dataset to compare different techniques.  I am not doing GPU training since its too expensive, both computationally and in my wallet!.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparisons'></a>\n",
    "## Comparing Keras Models\n",
    "Below are several models I trained and experimented with.  \n",
    "\n",
    "These models were trained on the same data.  I trained each models to test on 3 different days, 1/1/16,1/2/16, and 1/3/16.  The validation set consisted of the previous day from the test set.  The training set consisted of the previous 75 days worth of data, with each row containing 6 days worth of data.  Since each day consists of 144 records, the datasets np.shape looks like the following: \n",
    "\n",
    "#### Training Set: \n",
    "(10656, 864, 3)<br>\n",
    "75 days*144 records each day-144 records for validation set=10,656 rows.  <br>\n",
    "6 days columns *144 each day-864 columns<br>\n",
    "3 variables for each point (wind direction, wind speed, outdoor temperature)<br>\n",
    "\n",
    "##### Test, Validation Sets\n",
    "(144, 864, 3)<br>\n",
    "144 records in a day<br>\n",
    "6 days columns *144 each day-864 columns <br>\n",
    "3 variables for each point (wind direction, wind speed, outdoor temperature)<br><br>\n",
    "\n",
    "\n",
    "## Model notes\n",
    "Constants: adam optimization, training/test size, Dropout Regularization=.2 after each layer.  \n",
    "\n",
    "|Model|Type|Layers|Nodes|Loss Function|Shuffle|stateful/reset (Y/N)|\n",
    "|------|------|------|------|------|------|------|\n",
    "|1|LSTM|1|16|mae|False|N|\n",
    "|2|LSTM|1|8||mae|False|N|\n",
    "|3|LSTM|1|32|mae|False|N|\n",
    "|4|LSTM|2|4,4|mae|False|N|\n",
    "|5|LSTM|1|4|rmse|False|N|\n",
    "|6|LSTM|1|8|mae|False|Y|\n",
    "|7|LSTM|1|8|rmse|True|N|\n",
    "|8|LSTM|1|4|rmse|True|N|\n",
    "|9|LSTM|1|8|mae|False|Y|\n",
    "|10|LSTM|1|8|rmse|False|Y|\n",
    "\n",
    "\n",
    "\n",
    "Models 1-5 were normal, stateless LSTM's with differing nodes, layers, and loss functions.  MAE seemed to consistently do better during my training.  <br>\n",
    "\n",
    "Models 7-8 were stateless LSTMs with shuffling set to True.  \n",
    "\n",
    "Models 6, 9-10 are stateful LSTM's.  I seemed to have the most success with these according to the results below. \n",
    "\n",
    "### Stateful vs. Stateless LSTM\n",
    "[This link](https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/) was really helpful in understanding difference between stateful and stateless LSTMs, and how we should use each.  \n",
    "#### Stateless\n",
    "The state of the model is reset after each training batch.\n",
    "\n",
    "#### Stateful\n",
    "The state of the model is reset only when reset_state() is called in keras.  For my models, I am resetting after each epoch.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import  CuDNNLSTM\n",
    "from keras import optimizers\n",
    "\n",
    "#adam optimizer with default rates.  \n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "#Model 1\n",
    "model.add(LSTM(16, input_shape=(testX.shape[1], testX.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_absolute_error', optimizer=adam)\n",
    "\n",
    "#model 2\n",
    "model.add(LSTM(8, input_shape=(testX.shape[1], testX.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "#model 3\n",
    "model.add(LSTM(4, input_shape=(testX.shape[1], testX.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_absolute_error', optimizer=adam)\n",
    "\n",
    "#model 4\n",
    "model.add(LSTM(4, input_shape=(testX.shape[1], testX.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(4))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "#model 5\n",
    "model.add(LSTM(4, input_shape=(testX.shape[1], testX.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "\n",
    "#Model 6\n",
    "model.add(LSTM(8, batch_input_shape=(testX.shape[0],testX.shape[1], testX.shape[2]), stateful=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "for i in range(5):\n",
    "    model.fit(trainX_initial, trainY_initial, validation_data=(validationX, validationY),epochs=30, batch_size=testX.shape[0], verbose=2, shuffle=False)\n",
    "    model.reset_states()\n",
    "\n",
    "\n",
    "#model 7-shuffle true\n",
    "model.add(LSTM(8, input_shape=(testX.shape[1], testX.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "model.fit(trainX_initial, trainY_initial, validation_data=(validationX, validationY),epochs=30, batch_size=testX.shape[0], verbose=2, shuffle=True)\n",
    "\n",
    "\n",
    "#model 8-shuffle true\n",
    "model.add(LSTM(4, input_shape=(testX.shape[1], testX.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "                \n",
    "#model 9\n",
    "model.add(LSTM(8, batch_input_shape=(testX.shape[0],testX.shape[1], testX.shape[2]), stateful=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "for i in range(5):\n",
    "        model.fit(trainX_initial, trainY_initial, validation_data=(validationX, validationY),epochs=10, batch_size=testX.shape[0], verbose=2, shuffle=False)\n",
    "        model.reset_states()\n",
    "\n",
    "\n",
    "#model 10\n",
    "#stateful\n",
    "model.add(LSTM(8, batch_input_shape=(testX.shape[0],testX.shape[1], testX.shape[2]), stateful=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "for i in range(5):\n",
    "        model.fit(trainX_initial, trainY_initial, validation_data=(validationX, validationY),epochs=10, batch_size=testX.shape[0], verbose=2, shuffle=False)\n",
    "        model.reset_states()\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Callbacks\n",
    "\n",
    "I added the following callbacks to the training on each of the stateless models:<br>\n",
    "ReduceLROnPlateau: If the validation set loss doesn't decrease in 2 epochs I will halve the learning rate.  <br>\n",
    "EarlyStopping:  If the validation loss doesn't improve in 5 epochs, I will end the training.  Minimum change is 0. <br>\n",
    "ModelCheckpoint: Only save weights for the model when the validation loss has improved.  \n",
    "<br><br>\n",
    "Training:  I feed the training method training and validation data with a batch size=144, no shuffling, and the list of callbacks above.  \n",
    "Code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001, verbose=1)\n",
    "checkpointer=ModelCheckpoint('weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "earlystopper=EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "model.fit(trainX_initial, trainY_initial, validation_data=(validationX, validationY),epochs=30, batch_size=testX.shape[0], verbose=2, shuffle=False,callbacks=[checkpointer, earlystopper, reduce_lr])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "\n",
    "|Date|Model 1|mae|rmse|Model 2|mae|rmse|Model 3|mae|rmse|Model 4|mae|rmse|\n",
    "|------|------|------|------|------|------|------|------|------|------|------|------|------|\n",
    "|2016-01-01|-|11.132833|15.008581|-|12.995749|18.013858|-|15.819534|20.218835|-|18.058130|23.207323|\n",
    "|2016-01-02|-|4.215699|5.686442|-|5.644555|7.325435|-|4.208258|5.581017|-|5.471353 |7.309605|\n",
    "|2016-01-03|-|6.929635|9.113529|-|9.854110| 11.997647|-|8.457990|10.596992|-|10.320796|13.243282|\n",
    "\n",
    "\n",
    "|Date|Model 5:|mae|rmse|Model 6:|mae|rmse|Model 7:|mae|rmse|\n",
    "|------|------|------|------|------|------|------|------|------|------|\n",
    "|2016-01-01|-|17.856244|22.631304|-|9.883107|13.322008|-|10.735319|15.443263|\n",
    "|2016-01-02|-|5.151393|6.878382|-|3.343367|4.565645|-|4.928734|6.508967|\n",
    "|2016-01-03|-|9.248170|11.711768|-|5.457156|7.486818|-|6.826818|9.645841|\n",
    "\n",
    "\n",
    "|Date|Model 8|degrees_mae|rmse|Model 9|degrees_mae|rmse|Model 10|degrees_mae|rmse|\n",
    "|------|------|------|------|------|------|------|------|------|------|\n",
    "|2016-01-01|-|11.104123|16.318785|-|11.031389|14.650800|-|12.237451|16.953266|\n",
    "|2016-01-02|-|4.702615|6.148375|-|3.519534|4.774981|-|5.372925|7.099109|\n",
    "|2016-01-03|-|8.826431|11.568850|-|5.257079|7.249314|-|9.937034|12.236369|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Models: 6, 9, 3, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='ensembling'></a>\n",
    "# Ensembling\n",
    "\n",
    "I attempted to ensemble 4 models to an Average() layer in Keras, however [I ran into issues](https://github.com/keras-team/keras/issues/9385).  <br>\n",
    "\n",
    "There is currently a bug when stacking stateful submodels in a keras ensemble.  I instead stacked 4 stateless models manually.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking method:\n",
    "1.  Train 4 models on sine data and 4 models on cosine data\n",
    "2.  Retrieve the validation set rmse and mae for each model.  \n",
    "3.  Ensemble 4 sine model predictions together by weighting them using RMSE or MAE:\n",
    "    - The lower the error on a validation set for a model, the more weight its predictions receive.  \n",
    "    -  Code for this is in the method 'weightedPredictions' below.  \n",
    "4.  Do the same for 4 cosine models.\n",
    "5.  Ensemble sine and cosine ensembles together using the same method I have been using all along. \n",
    "\n",
    "My results didn't improve predictions.  \n",
    "Ideas for enhancement:\n",
    "-  More variance in models\n",
    "-  Ensemble with persistance model\n",
    "\n",
    "\n",
    "#### Results\n",
    "#### Model 1\n",
    "MAE weighted: 9.296 rmse: 14.09<br>\n",
    "RMSE weighted mae: 9.409301262546116 rmse: 14.128<br>\n",
    "#### Model 2\n",
    "MAE weighted: 9.64 rmse: 14.314<br>\n",
    "RMSE weighted mae: 9.77 rmse: 14.39<br>\n",
    "#### Model 3\n",
    "MAE weighted: 9.34 rmse: 14.06<br>\n",
    "RMSE weighted mae:9.51 rmse: 14.14<br>\n",
    "#### Model 4\n",
    "MAE weighted: 8.63 rmse: 13.53<br>\n",
    "RMSE weighted mae:8.66 rmse: 13.5<br>\n",
    "#### Ensembled\n",
    "MAE weighted: 9.30 rmse: 14.01<br>\n",
    "RMSE weighted mae:9.32 rmse: 14.02<br>\n",
    "\n",
    "A little bit of the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(actual,predicted):\n",
    "        mae=mean_absolute_error(actual, predicted)\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        return mae, rmse\n",
    "\n",
    "def weightedPredictions(predictions,errors):\n",
    "        inverseErrorTotal=0\n",
    "        for e in errors:\n",
    "                inverseErrorTotal+=(sum(errors)-e)\n",
    "        weighted= np.zeros(predictions[0].shape)\n",
    "        for p,e in zip(predictions,errors):\n",
    "                prediction_weight=(sum(errors)-e)/inverseErrorTotal\n",
    "                weighted+=(p*prediction_weight)\n",
    "        return weighted\n",
    "\n",
    "results = pd.DataFrame(columns=['test_date','degrees_mae','rmse'])\n",
    "\n",
    "\n",
    "date_to_test=datetime.datetime(2016, 1, 1)\n",
    "currentTurbine,total,recordsBack, trainSet=dataSetup(date_to_test)\n",
    "\n",
    "trainX_initial, trainY_initial, validationX, validationY, testX,testY,actual=setupTrainTestSets(currentTurbine,total,recordsBack, trainSet)\n",
    "testPredict_sin_1,validation_sin1=train_predict(1,False)\n",
    "testPredict_sin_2,validation_sin2=train_predict(2,False)\n",
    "testPredict_sin_3,validation_sin3=train_predict(3,False)\n",
    "testPredict_sin_4,validation_sin4=train_predict(4,False)\n",
    "\n",
    "val_mae_sin_1,val_rmse_sin_1=errors(validationY,validation_sin1)\n",
    "val_mae_sin_2,val_rmse_sin_2=errors(validationY,validation_sin2)\n",
    "val_mae_sin_3,val_rmse_sin_3=errors(validationY,validation_sin3)\n",
    "val_mae_sin_4,val_rmse_sin_4=errors(validationY,validation_sin4)\n",
    "\n",
    "val_ensemble_sin_mae=weightedPredictions([validation_sin1,validation_sin2,validation_sin3,validation_sin4],[val_mae_sin_1,val_mae_sin_2,val_mae_sin_3,val_mae_sin_4])\n",
    "val_ensemble_sin_rmse=weightedPredictions([validation_sin1,validation_sin2,validation_sin3,validation_sin4],[val_rmse_sin_1,val_rmse_sin_2,val_rmse_sin_3,val_rmse_sin_4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='validation'></a>\n",
    "\n",
    "# Better Validation Testing\n",
    "![Backtesting](https://cdn-images-1.medium.com/max/2000/1*ETxu0I0BXP1M45pRRfph_Q.png)\n",
    "Link from: https://medium.com/cindicator/backtesting-time-series-models-weekend-of-a-data-scientist-92079cc2c540\n",
    "\n",
    "Up until now, I've been doing validation on a single day of data.  On top of this, I've been saving the models weights based on loss of this single dataset of 144 values.  In this exercise I built iterative time stepping of the training and test sets.  Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict():\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(8, input_shape=(trainX.shape[1],trainX.shape[2])))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=2, min_lr=0.000001, verbose=1)                                  \n",
    "        earlystopper=EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "        validatons=3\n",
    "        for i in reversed(range (1,validatons)):\n",
    "            if(cos):\n",
    "                weightFile='weights'+str(i)+'_cos.h5'                \n",
    "            else:\n",
    "                weightFile='weights'+str(i)+'_sin.h5'\n",
    "            checkpointer=ModelCheckpoint(weightFile, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "            model.fit(trainX[:-144*i], trainY[:-144*i], validation_data=(trainX[-144*i:len(trainX)-144*(i-1)], trainY[-144*i:len(trainX)-144*(i-1)]),epochs=25, batch_size=testX.shape[0], verbose=2, shuffle=False,callbacks=[checkpointer,earlystopper])\n",
    "\n",
    "        testPredict=[]\n",
    "        validationPredict=[]\n",
    "        for i in reversed(range (1,validatons)):\n",
    "                if(cos):\n",
    "                        weightFile='weights'+str(i)+'_cos.h5'                \n",
    "                else:\n",
    "                        weightFile='weights'+str(i)+'_sin.h5'               \n",
    "\n",
    "                model.load_weights(weightFile)\n",
    "                validationPredict.append(model.predict(validationX,batch_size=testX.shape[0]))\n",
    "                testPredict.append(model.predict(testX, batch_size=testX.shape[0]))                   \n",
    "                \n",
    "        average_test=sum(testPredict) / float(len(testPredict))\n",
    "        average_val=sum(validationPredict) / float(len(validationPredict))\n",
    "        return average_test, average_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results here were inconclusive, but overall the method seems to be a bit better than validating on a single validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='weights'></a>\n",
    "# Weight Initialization\n",
    "\n",
    "I learned from previous projects that weight initialization matters quite a bit.  This is why many Kaggle competitions are won using pre-trained networks.  In Keras, we can load weights from successful pre-trained models, however I have never done this with an LSTM. \n",
    "\n",
    "Default weight initializations for Keras are:<br>\n",
    "kernel_initializer='glorot_uniform'<br>\n",
    "recurrent_initializer='orthogonal'<br>\n",
    "\n",
    "Initializations as defined on [Keras website](https://keras.io/layers/recurrent/)\n",
    "#### kernel_initializer: \n",
    "Initializer for the kernel weights matrix, used for the linear transformation of the inputs. <br>\n",
    "#### recurrent_initializer: \n",
    "Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. <br> <br>\n",
    "I tried the following weight initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(model_num==1):\n",
    "        model.add(LSTM(16, batch_input_shape=(testX.shape[0],testX.shape[1], testX.shape[2]),kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal'))\n",
    "if(model_num==2):\n",
    "        model.add(LSTM(16, batch_input_shape=(testX.shape[0],testX.shape[1], testX.shape[2]),kernel_initializer='glorot_normal', recurrent_initializer='orthogonal'))\n",
    "if(model_num==3):\n",
    "        model.add(LSTM(16, batch_input_shape=(testX.shape[0],testX.shape[1], testX.shape[2]),kernel_initializer='glorot_uniform', recurrent_initializer='glorot_normal'))\n",
    "if(model_num==4):\n",
    "        model.add(LSTM(16, batch_input_shape=(testX.shape[0],testX.shape[1], testX.shape[2]),kernel_initializer='he_normal', recurrent_initializer='orthogonal'))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "|Model|mae|rmse|\n",
    "|------|------|------|\n",
    "|1 |9.05|12.84|\n",
    "|2|10.03|14.58|\n",
    "|3|9.61|14.10|\n",
    "|4|9.21|13.06|\n",
    "\n",
    "\n",
    "#### Conclusions\n",
    "I need to do more experimentation with different combinations of the different initializers.  Additionally, I only trained these for 50 epochs.  I would want to train them for 100 epochs to see if they are able to train and converge better with these different initialized weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "# Conclusions and Future Considerations\n",
    "\n",
    "Hard lessons were learned in this exercise.  During my previous project I threw expensive GPU at this problem.  However I found that the persistance model performed better than the LSTM models.   I do believe the above techniques can be combined together for better performance, however, this is a hard problem to solve.  Something significant I learned towards the end of my project was that I haven't been training my models long enough.  Additionally, updating weights only on the validation set was the wrong approach.  Here is a simple model that performed better than my other models, trained using 365 days of row data and 6 previous days of column data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict():\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(16, batch_input_shape=(testX.shape[0],testX.shape[1], testX.shape[2])))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.9, patience=2, min_lr=0.000001, verbose=1)         \n",
    "        checkpointer=ModelCheckpoint('weights.h5', monitor='loss', verbose=2, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "        earlystopper=EarlyStopping(monitor='loss', min_delta=0, patience=8, verbose=0, mode='auto')\n",
    "\n",
    "        model.fit(trainX_full, trainY_full, validation_data=(validationX, validationY),epochs=100, batch_size=testX.shape[0], verbose=2, shuffle=False,callbacks=[checkpointer, earlystopper,reduce_lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences:\n",
    "-  Only 16 nodes\n",
    "-  Training for 100 epochs\n",
    "-  Updating weights based on lowest loss rather than lowest validation loss.  \n",
    "\n",
    "### Results:\n",
    "|Date|degrees_mae|rmse|\n",
    "|------|------|------|\n",
    "|2016-01-01 |7.83|11.19|\n",
    "|2016-01-02 |3.12|4.42|\n",
    "|2016-01-03 |4.49|6.23|\n",
    "\n",
    "I believe some combination of the following techniques would further help the forecasting models:\n",
    "-  Ensembling stateful and stateless and alternatively structured models into a single prediction.  \n",
    "-  Possible alternative weight initializations. \n",
    "-  Possible ensembling with the persistence model.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='numpylstm'></a>\n",
    "\n",
    "# Part 2\n",
    "\n",
    "# LSTM in only NumPy\n",
    "\n",
    "I wanted to understand the inner workings of an LSTM Recurrent Neural Network which led me to scour the internet for links.  The best link by far is [Christopher Olah's blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n",
    "\n",
    "I attempted to implement the network, but because of a lack of time and lack of skill I had to give up.  Instead, I found Nick Jimenez's python classes to be highly useful:\n",
    "NumPy taken from [Nicolas D. Jimenez's GitHub](https://github.com/nicodjimenez/lstm/blob/master/lstm.py).  \n",
    "\n",
    "I used 10 rows from my wind direction dataset and converted the wind direction to a sine wave.  \n",
    "\n",
    "#### Adam Optimization\n",
    "I implemented Adam optimization to Nick's network rather than using SGD.  I used a GitHub page to help me write this, but I can't seem to find the link now.  Below I graphed a learning comparison of Adam vs SGD, both with a learning rate of 0.1. \n",
    "\n",
    "An interesting note on Adam optimization was that the network was only able to learn well with Adam optimization after initializing weights to <br>\n",
    "(np.random.rand(*args) * (.2) -.1)*.1<br>\n",
    "Very low weight initialization was required for the network to learn the data with Adam; Adam optimization only worked well after the \"*.1\" was added.  Adam optimization did however learn more effectively than SGD after this modification.  With Nick's original SGD weight initialization, the final loss on the network with this data was 0.18.  After adjusting the weight initialization, SGD loss was .26 and Adam loss was .0013.\n",
    "Training SGD for more iterations would make up for its learning deficiencies compared to Adam.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turbine name: R80736\n",
      "Training...\n",
      "iter  0: y_pred = [-0.00022,  0.00051,  0.00034,  0.00070,  0.00028,  0.00122], loss: 3.332e+00\n",
      "iter 50: y_pred = [ 0.47206, -0.16051,  0.51563,  0.52871, -0.54239, -0.49950], loss: 8.613e-01\n",
      "Actual vs Predicted:\n",
      "[ 0.64447554 -0.92569097  0.6678121   0.9735639  -0.68801814 -0.43752836]\n",
      "[0.5869153998588292, -0.4670886537363172, 0.5520156916916225, 0.7768378584348697, -0.7074944282921246, -0.4815653387488001]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "iter  0: y_pred = [-0.00022,  0.00051,  0.00034,  0.00070,  0.00028,  0.00122], loss: 3.332e+00\n",
      "iter 50: y_pred = [ 0.62422, -0.83417,  0.63624,  0.81116, -0.69211, -0.53279], loss: 4.525e-02\n",
      "Actual vs Predicted:\n",
      "[ 0.64447554 -0.92569097  0.6678121   0.9735639  -0.68801814 -0.43752836]\n",
      "[0.6452756994939641, -0.9033736746297787, 0.6690755544839511, 0.9535165763784924, -0.6837271453973008, -0.41737458799120275]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class Optimizer:\n",
    "    #USE SAME DEFAULTS AS KERAS ADAM OPTIMIZER\n",
    "    def __init__(self, lr=.1, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=0, decay=0., **kwargs):\n",
    "        \n",
    "        allowed_kwargs = {'clipnorm', 'clipvalue'}\n",
    "        for k in kwargs:\n",
    "            if k not in allowed_kwargs:\n",
    "                raise TypeError('Unexpected keyword argument '\n",
    "                                'passed to optimizer: ' + str(k))\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.iterations = 1\n",
    "        self.lr = lr\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    def get_ADAM(self, params, grads):\n",
    "\n",
    "        original_shapes = [x.shape for x in params]\n",
    "        params = [x.flatten() for x in params]\n",
    "        grads = [x.flatten() for x in grads]\n",
    "        \n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "        lr_t = lr * (np.sqrt(1. - np.power(self.beta_2, t)) /\n",
    "                     (1. - np.power(self.beta_1, t)))\n",
    "\n",
    "        if not hasattr(self, 'ms'):\n",
    "            self.ms = [np.zeros(p.shape) for p in params]\n",
    "            self.vs = [np.zeros(p.shape) for p in params]\n",
    "    \n",
    "        ret = [None] * len(params)\n",
    "        for i, p, g, m, v in zip(range(len(params)), params, grads, self.ms, self.vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * np.square(g)\n",
    "            p_t = p - lr_t * m_t / (np.sqrt(v_t) + self.epsilon)\n",
    "            self.ms[i] = m_t\n",
    "            self.vs[i] = v_t\n",
    "            ret[i] = p_t\n",
    "        self.iterations += 1\n",
    "  \n",
    "        for i in range(len(ret)):\n",
    "            ret[i] = ret[i].reshape(original_shapes[i])\n",
    "\n",
    "        return np.array(ret)\n",
    "\n",
    "\n",
    "    def get_SGD(self, w,p):\n",
    "        for x,y in zip(w,p):\n",
    "                    x+=self.lr*y\n",
    "        return w[0],w[1],w[2],w[3],w[4],w[5],w[6],w[7],w[8],w[9]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(values): \n",
    "    return values*(1-values)\n",
    "\n",
    "def tanh_derivative(values): \n",
    "    return 1. - values ** 2\n",
    "\n",
    "# createst uniform random array w/ values in [a,b) and shape args\n",
    "def rand_arr(a, b, *args): \n",
    "    np.random.seed(0)\n",
    "    return (np.random.rand(*args) * (b - a) + a)*.1\n",
    "\n",
    "class LstmParam:\n",
    "    def __init__(self, mem_cell_ct, x_dim,optimization):\n",
    "        self.mem_cell_ct = mem_cell_ct\n",
    "        self.x_dim = x_dim\n",
    "        concat_len = x_dim + mem_cell_ct\n",
    "        \n",
    "        self.opt=Optimizer()\n",
    "        self.optimization=optimization\n",
    "\n",
    "        # weight matrices\n",
    "        self.wg = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)\n",
    "        self.wi = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len) \n",
    "        self.wf = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)\n",
    "        self.wo = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)\n",
    "\n",
    "        # bias terms\n",
    "        self.bg = rand_arr(-0.1, 0.1, mem_cell_ct) \n",
    "        self.bi = rand_arr(-0.1, 0.1, mem_cell_ct) \n",
    "        self.bf = rand_arr(-0.1, 0.1, mem_cell_ct) \n",
    "        self.bo = rand_arr(-0.1, 0.1, mem_cell_ct)\n",
    "\n",
    "\n",
    "        \n",
    "        # diffs (derivative of loss function w.r.t. all parameters)\n",
    "        self.wg_diff = np.zeros((mem_cell_ct, concat_len)) \n",
    "        self.wi_diff = np.zeros((mem_cell_ct, concat_len)) \n",
    "        self.wf_diff = np.zeros((mem_cell_ct, concat_len)) \n",
    "        self.wo_diff = np.zeros((mem_cell_ct, concat_len)) \n",
    "        self.bg_diff = np.zeros(mem_cell_ct) \n",
    "        self.bi_diff = np.zeros(mem_cell_ct) \n",
    "        self.bf_diff = np.zeros(mem_cell_ct) \n",
    "        self.bo_diff = np.zeros(mem_cell_ct) \n",
    "\n",
    "    def apply_diff(self, lr = .1):\n",
    "        if(self.optimization=='adam'):\n",
    "            self.wg=self.opt.get_ADAM(self.wg,self.wg_diff)\n",
    "            self.wi=self.opt.get_ADAM(np.array(self.wi),np.array(self.wi_diff))\n",
    "            self.wf=self.opt.get_ADAM(np.array(self.wf),np.array(self.wf_diff))\n",
    "            self.wo=self.opt.get_ADAM(np.array(self.wo),np.array(self.wo_diff))\n",
    "\n",
    "        else:\n",
    "            #This is the stochastic gradient descent code\n",
    "            self.wg -= lr * self.wg_diff\n",
    "            self.wi -= lr * self.wi_diff\n",
    "            self.wf -= lr * self.wf_diff\n",
    "            self.wo -= lr * self.wo_diff\n",
    "\n",
    "\n",
    "        \n",
    "        self.bg -= lr * self.bg_diff\n",
    "        self.bi -= lr * self.bi_diff\n",
    "        self.bf -= lr * self.bf_diff\n",
    "        self.bo -= lr * self.bo_diff\n",
    "        \n",
    "        # reset diffs to zero\n",
    "        self.wg_diff = np.zeros_like(self.wg)\n",
    "        self.wi_diff = np.zeros_like(self.wi) \n",
    "        self.wf_diff = np.zeros_like(self.wf) \n",
    "        self.wo_diff = np.zeros_like(self.wo) \n",
    "        self.bg_diff = np.zeros_like(self.bg)\n",
    "        self.bi_diff = np.zeros_like(self.bi) \n",
    "        self.bf_diff = np.zeros_like(self.bf) \n",
    "        self.bo_diff = np.zeros_like(self.bo) \n",
    "\n",
    "class LstmState:\n",
    "    def __init__(self, mem_cell_ct, x_dim):\n",
    "        self.g = np.zeros(mem_cell_ct)\n",
    "        self.i = np.zeros(mem_cell_ct)\n",
    "        self.f = np.zeros(mem_cell_ct)\n",
    "        self.o = np.zeros(mem_cell_ct)\n",
    "        self.s = np.zeros(mem_cell_ct)\n",
    "        self.h = np.zeros(mem_cell_ct)\n",
    "        self.bottom_diff_h = np.zeros_like(self.h)\n",
    "        self.bottom_diff_s = np.zeros_like(self.s)\n",
    "    \n",
    "class LstmNode:\n",
    "    def __init__(self, lstm_param, lstm_state):\n",
    "        # store reference to parameters and to activations\n",
    "        self.state = lstm_state\n",
    "        self.param = lstm_param\n",
    "\n",
    "        # non-recurrent input concatenated with recurrent input\n",
    "        self.xc = None\n",
    "\n",
    "    def bottom_data_is(self, x, s_prev = None, h_prev = None):\n",
    "        # if this is the first lstm node in the network\n",
    "        if s_prev is None: s_prev = np.zeros_like(self.state.s)\n",
    "        if h_prev is None: h_prev = np.zeros_like(self.state.h)\n",
    "        # save data for use in backprop\n",
    "        self.s_prev = s_prev\n",
    "        self.h_prev = h_prev\n",
    "\n",
    "        # concatenate x(t) and h(t-1)\n",
    "        xc = np.hstack((x,  h_prev))\n",
    "        self.state.g = np.tanh(np.dot(self.param.wg, xc) + self.param.bg)\n",
    "        self.state.i = sigmoid(np.dot(self.param.wi, xc) + self.param.bi)\n",
    "        self.state.f = sigmoid(np.dot(self.param.wf, xc) + self.param.bf)\n",
    "        self.state.o = sigmoid(np.dot(self.param.wo, xc) + self.param.bo)\n",
    "        self.state.s = self.state.g * self.state.i + s_prev * self.state.f\n",
    "        self.state.h = self.state.s * self.state.o\n",
    "\n",
    "        self.xc = xc\n",
    "\n",
    "    \n",
    "    def top_diff_is(self, top_diff_h, top_diff_s):\n",
    "        # notice that top_diff_s is carried along the constant error carousel\n",
    "        ds = self.state.o * top_diff_h + top_diff_s\n",
    "        do = self.state.s * top_diff_h\n",
    "        di = self.state.g * ds\n",
    "        dg = self.state.i * ds\n",
    "        df = self.s_prev * ds\n",
    "\n",
    "        # diffs w.r.t. vector inside sigma / tanh function\n",
    "        di_input = sigmoid_derivative(self.state.i) * di \n",
    "        df_input = sigmoid_derivative(self.state.f) * df \n",
    "        do_input = sigmoid_derivative(self.state.o) * do \n",
    "        dg_input = tanh_derivative(self.state.g) * dg\n",
    "\n",
    "        # diffs w.r.t. inputs\n",
    "        self.param.wi_diff += np.outer(di_input, self.xc)\n",
    "        self.param.wf_diff += np.outer(df_input, self.xc)\n",
    "        self.param.wo_diff += np.outer(do_input, self.xc)\n",
    "        self.param.wg_diff += np.outer(dg_input, self.xc)\n",
    "        self.param.bi_diff += di_input\n",
    "        self.param.bf_diff += df_input       \n",
    "        self.param.bo_diff += do_input\n",
    "        self.param.bg_diff += dg_input\n",
    "\n",
    "        #for dparam in [self.param.wi_diff, self.param.wf_diff , self.param.wo_diff, self.param.wg_diff, self.param.bi_diff, self.param.bf_diff, self.param.bo_diff, self.param.bg_diff]:\n",
    "        #    np.clip(dparam, -1, 1, out=dparam)\n",
    "\n",
    "        # compute bottom diff\n",
    "        dxc = np.zeros_like(self.xc)\n",
    "        dxc += np.dot(self.param.wi.T, di_input)\n",
    "        dxc += np.dot(self.param.wf.T, df_input)\n",
    "        dxc += np.dot(self.param.wo.T, do_input)\n",
    "        dxc += np.dot(self.param.wg.T, dg_input)\n",
    "\n",
    "        # save bottom diffs\n",
    "        self.state.bottom_diff_s = ds * self.state.f\n",
    "        self.state.bottom_diff_h = dxc[self.param.x_dim:]\n",
    "\n",
    "class LstmNetwork():\n",
    "    def __init__(self, lstm_param, loss):\n",
    "        self.lstm_param = lstm_param\n",
    "        self.lstm_node_list = []\n",
    "        # input sequence\n",
    "        self.x_list = []\n",
    "        self.loss=loss\n",
    "\n",
    "    def y_list_is(self, y_list, loss_layer):\n",
    "        \"\"\"\n",
    "        Updates diffs by setting target sequence \n",
    "        with corresponding loss layer. \n",
    "        Will *NOT* update parameters.  To update parameters,\n",
    "        call self.lstm_param.apply_diff()\n",
    "        \"\"\"\n",
    "        assert len(y_list) == len(self.x_list)\n",
    "        idx = len(self.x_list) - 1\n",
    "        # first node only gets diffs from label ...\n",
    "        loss = loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx],self.loss)\n",
    "\n",
    "        diff_h =loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])\n",
    "\n",
    "        # here s is not affecting loss due to h(t+1), hence we set equal to zero\n",
    "        diff_s = np.zeros(self.lstm_param.mem_cell_ct)\n",
    "        self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)\n",
    "        idx -= 1\n",
    "\n",
    "        ### ... following nodes also get diffs from next nodes, hence we add diffs to diff_h\n",
    "        ### we also propagate error along constant error carousel using diff_s\n",
    "        while idx >= 0:\n",
    "            loss += loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx],self.loss)\n",
    "            diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])\n",
    "            diff_h += self.lstm_node_list[idx + 1].state.bottom_diff_h\n",
    "            diff_s = self.lstm_node_list[idx + 1].state.bottom_diff_s\n",
    "            self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)\n",
    "            idx -= 1 \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def x_list_clear(self):\n",
    "        self.x_list = []\n",
    "\n",
    "    def x_list_add(self, x):\n",
    "        self.x_list.append(x)\n",
    "       # print(self.x_list)\n",
    "        if len(self.x_list) > len(self.lstm_node_list):\n",
    "            # need to add new lstm node, create new state mem\n",
    "            lstm_state = LstmState(self.lstm_param.mem_cell_ct, self.lstm_param.x_dim)\n",
    "            self.lstm_node_list.append(LstmNode(self.lstm_param, lstm_state))\n",
    "\n",
    "        # get index of most recent x input\n",
    "        idx = len(self.x_list) - 1\n",
    "        if idx == 0:\n",
    "            # no recurrent inputs yet\n",
    "            self.lstm_node_list[idx].bottom_data_is(x)\n",
    "        else:\n",
    "            s_prev = self.lstm_node_list[idx - 1].state.s\n",
    "            h_prev = self.lstm_node_list[idx - 1].state.h\n",
    "            self.lstm_node_list[idx].bottom_data_is(x, s_prev, h_prev)\n",
    "\n",
    "\n",
    "\n",
    "class LossLayer:\n",
    "    \"\"\"\n",
    "    Computes square loss with first element of hidden layer array.\n",
    "    MG-Attempted to add in mae loss for comparison, but RMSE and MAE loss performed the same.  \n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def loss(self,pred, label,fn):\n",
    "        if(fn=='mae'):\n",
    "            return LossLayer.loss_mae(pred,label)\n",
    "        else:\n",
    "            return LossLayer.loss_rmse(pred,label)\n",
    "    \n",
    "    # MG added mean absolute error\n",
    "    @classmethod\n",
    "    def loss_mae(self, pred, label):\n",
    "        return (np.abs(pred[0]-label))\n",
    "        #return (pred[0] - label) ** 2\n",
    "    \n",
    "    @classmethod\n",
    "    def loss_rmse(self, pred, label):\n",
    "        return (pred[0] - label) ** 2\n",
    "\n",
    "    @classmethod\n",
    "    def bottom_diff(self, pred, label):\n",
    "        diff = np.zeros_like(pred)\n",
    "        diff[0] =2*(pred[0] - label)\n",
    "        return diff\n",
    "\n",
    "\n",
    "\n",
    "def train(loss, optimization):\n",
    "    mem_cell_ct = 50\n",
    "    x_dim = 4\n",
    "    lstm_param = LstmParam(mem_cell_ct, x_dim,optimization)\n",
    "    lstm_net = LstmNetwork(lstm_param,loss)\n",
    "    losses=[]\n",
    "    bestLoss=1e5\n",
    "    print(\"Training...\")\n",
    "    for cur_iter in range(100):\n",
    "       \n",
    "        for ind in range(len(Y)):\n",
    "            lstm_net.x_list_add(X[ind])\n",
    "\n",
    "        if(cur_iter%50==0):\n",
    "            print(\"iter\", \"%2s\" % str(cur_iter), end=\": \")\n",
    "            print(\"y_pred = [\" +\n",
    "                  \", \".join([\"% 2.5f\" % lstm_net.lstm_node_list[ind].state.h[0] for ind in range(len(Y))]) +\n",
    "                  \"]\", end=\", \")\n",
    "\n",
    "        loss = lstm_net.y_list_is(Y, LossLayer)\n",
    "        losses.append(loss)\n",
    "        if(loss<bestLoss):\n",
    "            best_lstm_net = LstmNetwork(lstm_param,loss)\n",
    "            \n",
    "        lstm_param.apply_diff(lr=0.1)\n",
    "        \n",
    "        if(cur_iter%50==0):\n",
    "            print(\"loss:\", \"%.3e\" % loss)\n",
    "\n",
    "        lstm_net.x_list_clear()\n",
    "    \n",
    "    for ind in range(len(Y)):\n",
    "        best_lstm_net.x_list_add(X[ind])   \n",
    "    loss = best_lstm_net.y_list_is(Y, LossLayer)\n",
    "    return losses, [ best_lstm_net.lstm_node_list[ind].state.h[0] for ind in range(len(Y))],loss\n",
    "\n",
    "\n",
    "\n",
    "def firstTurbineData():\n",
    "\tdf = pd.read_csv('la-haute-borne-data-2013-2016.csv', sep=';')\n",
    "\tdf['Date_time'] = df['Date_time'].astype(str).str[:-6] #remove timezone (caused me an hour of pain)\n",
    "\tdf.Date_time=pd.to_datetime(df['Date_time'])\n",
    "\tdf=df.fillna(method='ffill')\n",
    "\n",
    "\tdf=df.sort_values(by='Date_time')\n",
    "\tdf = df.reset_index()\n",
    "\tturbines=df.Wind_turbine_name.unique()\n",
    "\tprint(\"Turbine name: \"+str(turbines[0]))\n",
    "\tturbineData=df[df['Wind_turbine_name']==turbines[0]]\n",
    "\treturn turbineData\n",
    "\n",
    "\n",
    "def createGraph(losses, title):\n",
    "\tX = np.arange(0,len(losses))\n",
    "\tfigure = plt.figure()\n",
    "\ttick_plot = figure.add_subplot(1, 1, 1)\n",
    "\ttick_plot.plot(X, losses,  color='green', linestyle='-', marker='*' )\n",
    "\tplt.xlabel('Iteration')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.title(title)\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "date_to_test=datetime.datetime(2016, 1, 1)\n",
    "turbineData=np.sin(firstTurbineData().Wa_c_avg.values)[:10]\n",
    "X=np.array([turbineData[:4],\n",
    "                   turbineData[1:5],\n",
    "                   turbineData[2:6],\n",
    "                   turbineData[3:7],\n",
    "                   turbineData[4:8],\n",
    "                   turbineData[5:9]])\n",
    "Y=np.array([turbineData[4],\n",
    "                   turbineData[5],\n",
    "                   turbineData[6],\n",
    "                   turbineData[7],\n",
    "                   turbineData[8],\n",
    "                   turbineData[9]])\n",
    "\n",
    "\n",
    "losses, predictions,loss=train('rmse','sgd')\n",
    "print(\"Actual vs Predicted:\")\n",
    "print(Y)\n",
    "print(predictions)\n",
    "createGraph(losses,\"SGD Optimization\\nLoss=\"+str(loss))\n",
    "losses, predictions,loss=train('rmse','adam')\n",
    "print(\"Actual vs Predicted:\")\n",
    "print(Y)\n",
    "print(predictions)\n",
    "createGraph(losses,\"Adam Optimization\\nLoss=\"+str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='numpylstm_walkthrough'></a>\n",
    "## Code walkthrough\n",
    "\n",
    "Breakdown of an LSTM layer (from http://blog.varunajayasiri.com/numpy_lstm.html):\n",
    "![title](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
    "\n",
    "### Train Method Explanation: \n",
    "\n",
    "LstmParam:\n",
    "Initialize LstmParam object with hidden cell count (mem_cell_ct) and x_dim.  This object holds the networks weights, bias terms, and weight differences (derivative of loss function).  \n",
    "\n",
    "LstmNetwork:\n",
    "Holds Xlist for forward propagation. \n",
    "\n",
    "We will train the network for 100 iterations.  \n",
    "\n",
    "#### Forward propagation\n",
    "For each input value x[idx], I call the x_list_add method of the LstmNetwork:\n",
    "x_list_add:<br>\n",
    "If x_list > lstm_node_list:<br>\n",
    "- Create new LstmState.  The layers of the network should all be initialized to 0 arrays. \n",
    "- add LstmNode to lstm_node_list with LstmState with 0 arrays. \n",
    "\n",
    "If index of X input is 0:\n",
    "- Call lstm_node_list[idx].bottom_data_is(x) and forward propagate through the different inner modules of the LSTM Network.  LstmNode has \"states\" for each module, e.g. self.state.g, self.state.i.  state.s=C in the image above.  self.h and self.s get passed to the next input by being set via s_prev and h_prev, respectively. \n",
    "If index of X input is Not 0:\n",
    "- Call lstm_node_list[idx].bottom_data_is(x) and forward propagate through the different inner modules of the LSTM Network with the previous inputs s_prev and h_prev values.  \n",
    "\n",
    "Forward propagation yields the networks output guesses in its h[0] values, so we will print those periodically.  \n",
    "\n",
    "#### Calculating loss (y_list_is method):\n",
    "- input correct Y values.  To start, we take the last index from the lstm_node_list and calculate its loss by sending its h value to the loss function and returning the squared error of h[0] from its actual value, Y[idx].  \n",
    "\n",
    "- Calculate diff_h.  Find the error of the h[0] compared to the Y value.  I was unsure why Nick multiplied this error by 2, but it seemed to learn (at different rates) no matter what multiplication value you gave it.  Perhaps the 2 was to speed up learning by accentuating the loss? I'm not quite sure.  \n",
    "\n",
    "- For the last value, s (c) will be zero because it doesn't affect loss.\n",
    "\n",
    "- Backpropagate on final input value.  top_diff_is is self explanatory, computing the derivatives of each module in the LSTM layer.  \n",
    "\n",
    "- From here we backwards through the input data and perform the same back propagation: get the loss of the value compared to Y, get the loss of diff_h and diff_s, and backpropagate through the different modules.  diff_h is computed by finding the loss of the output value at the current index and then adding the loss of the value at the index+1.  diff_s is computed by getting the value of the diff_s at index+1.  \n",
    "\n",
    "#### Apply weights (apply_diff method):\n",
    "- Here is where I implemented Adam optimization.  We update weights and biases here by adding the diffs to the weights.  For example, self.wi weights are updated by adding the self.wi_diff*learning rate for SGD optimization. Adam optimization is different but follows a similar idea.  After the weights and biases are updated, we set the _diff variables back to zero.  \n",
    "\n",
    "From here we clear the x values and start another iteration. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rl'></a>\n",
    "\n",
    "\n",
    "# Part 3\n",
    "\n",
    "# Reinforcement Learning for the Multi Armed Bandit Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Multi-Armed Bandit Problem with Simple Reinforcement Learning\n",
    "\n",
    "\n",
    "The purpose of this exercise was to get my feet wet with simple reinforcement learning algorithms.  My goal was to write as simple code as possible for both learning purposes and readability.  <br>\n",
    "\n",
    "I solved the [multi-armed bandit problem](https://en.wikipedia.org/wiki/Multi-armed_bandit), a common machine learning problem.  I used a Bernoulli Bandit, which issues as reward of 1 with a probability of p and otherwise a reward of 0. \n",
    "\n",
    "Policies included:\n",
    "-  Random \n",
    "-  Greedy \n",
    "-  Epsilon \n",
    "-  Epsilon Decay \n",
    "-  Thompson Sampling\n",
    "-  Upper Confidence Bound (UCB)\n",
    "\n",
    "The \"Machine Learning\" part of this can be considered the dictionary Q, where I hold the reward values of each arm.  For example in epsilon greedy, if we pull an arm and win the reward, we increment the Q value of the that arm by:<br>\n",
    "Q[selection]+=((1/action_attempts[selection])*(1-Q[selection]))\n",
    "\n",
    "\n",
    "\n",
    "### Random\n",
    "The random approach is self explanatory.  We select a random arm from the bandit.  If we win with the random selection, I increment the score by 1.  \n",
    "\n",
    "### Greedy\n",
    "In the greedy approach, I choose the arm that has the highest reward.  Since all arms have zero rewards to begin with, it takes a random arm.  If this arm has the highest reward of all arms, then the greedy algorithm will do well.  However, most times it will lock onto an arm that is suboptimal and keep selecting this one.  It's reward will slowly increase, causing the algorithm to continue selecting this option.  This is not a great algorithm.  \n",
    "\n",
    "### Epsilon\n",
    "The epsilon algorithm selects a random value epsilon% of the time, and selects the value with the best reward (Q value) (1-epsilon) % of the time.  \n",
    "\n",
    "### Epsilon Decay\n",
    "Epsilon Decay lowers the epsilon value each iteration by a decay rate and follows the above logic.  This means that the further we get in the game, the more we should know about the rewards of each arm.  Therefore as epsilon decreases we will select the arm with the highest reward more and more.\n",
    "\n",
    "### Thompson Sampling\n",
    "Thompson Sampling uses the beta distribution.  We initalize alpha and beta to 1 because we are uncertain of each. If we are rewarded, we increment alpha by 1.  We add (1-reward) to beta.  [A smart guy on Quora](https://www.quora.com/What-is-Thompson-sampling-in-laymans-terms) explains Thompson Sampling well, and once I understood the beta distribution part of it, it was fairly easy to code.  \n",
    "\n",
    "### UCB\n",
    "I found a good explanation of the UCB algorithm in this link-https://stats.stackexchange.com/questions/323867/upper-confidence-bound-in-machine-learning<br>\n",
    "N is the total number sampled and n is the total of a particular bandit sampled.  C is a constant. I am unsure if the equation was implemented perfectly, but the results seem to be consistently strong.  \n",
    "![title](ucb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 armed bandit probabilities\n",
      "[0.13857907 0.71596157 0.4309315  0.02253543 0.66405794]\n",
      "\n",
      "Random Action\n",
      "{1: 0.1588785046728973, 2: 0.7499999999999998, 3: 0.5051546391752576, 4: 0.019801980198019806, 5: 0.6504854368932038}\n",
      "Score: 204\n",
      "\n",
      "Greedy\n",
      "{1: 0.162, 2: 0, 3: 0, 4: 0, 5: 0}\n",
      "Score: 81\n",
      "\n",
      "Epsilon Greedy No Decay\n",
      "{1: 0.1612903225806452, 2: 0.7428571428571432, 3: 0.5789473684210527, 4: 0.0, 5: 0.6933333333333334}\n",
      "Score: 328\n",
      "\n",
      "Epsilon Greedy with Decay\n",
      "{1: 0.0, 2: 0.0, 3: 0.41666666666666696, 4: 0.0, 5: 0.0}\n",
      "Score: 205\n",
      "\n",
      "Thompson Sampling\n",
      "Score: 328\n",
      "\n",
      "UCB\n",
      "[0.18181818181818185, 0.6923076923076923, 0.47058823529411764, 0.0, 0.7573770491803279]\n",
      "Score: 348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:151: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "random.seed()\n",
    "\n",
    "#10 armed bandit\n",
    "num_bandits=5\n",
    "mab=np.random.rand(num_bandits)\n",
    "print(str(num_bandits)+\" armed bandit probabilities\")\n",
    "print(mab)\n",
    "\n",
    "#number of pulls in game\n",
    "maxsteps=500\n",
    "\n",
    "\n",
    "def greedy():\n",
    "    best=max(Q.items(), key=operator.itemgetter(1))\n",
    "    return best[0]\n",
    "\n",
    "def random_selection():\n",
    "    return random.randint(1, num_bandits)\n",
    "\n",
    "def epsilonGreedy():\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Random Move\n",
    "        return random_selection()\n",
    "    else:\n",
    "        # Greedy Move\n",
    "        return greedy()\n",
    "\n",
    "#random \n",
    "print(\"\\nRandom Action\")\n",
    "Q={i:0 for i in range(1,num_bandits+1)}\n",
    "action_attempts={i:0 for i in range(1,num_bandits+1)}\n",
    "score=0\n",
    "rando=[]\n",
    "for step in range(maxsteps):\n",
    "    selection=random_selection()\n",
    "    action_attempts[selection]+=1    \n",
    "    reward=0\n",
    "    if(random.uniform(0, 1)<mab[selection-1]):\n",
    "        reward=1\n",
    "        score+=1\n",
    "    Q[selection]+=((1/action_attempts[selection])*(reward-Q[selection]))\n",
    "    rando.append(score)\n",
    "print(Q)\n",
    "print(\"Score: \"+str(score))\n",
    "\n",
    "\n",
    "\n",
    "#greedy\n",
    "print(\"\\nGreedy\")\n",
    "Q={i:0 for i in range(1,num_bandits+1)}\n",
    "action_attempts={i:0 for i in range(1,num_bandits+1)}\n",
    "score=0\n",
    "gre=[]\n",
    "for step in range(maxsteps):\n",
    "\n",
    "    selection=greedy()\n",
    "    action_attempts[selection]+=1\n",
    "    reward=0\n",
    "    if(random.uniform(0, 1)<mab[selection-1]):\n",
    "        score+=1\n",
    "        reward=1\n",
    "\n",
    "    Q[selection]+=((1/action_attempts[selection])*(reward-Q[selection]))\n",
    "    gre.append(score)\n",
    "print(Q)\n",
    "print(\"Score: \"+str(score))\n",
    "\n",
    "\n",
    "\n",
    "#epsilon greedy no decay\n",
    "print(\"\\nEpsilon Greedy No Decay\")\n",
    "epsilon=.2\n",
    "Q={i:0 for i in range(1,num_bandits+1)}\n",
    "action_attempts={i:0 for i in range(1,num_bandits+1)}\n",
    "score=0\n",
    "learningRate=0.2\n",
    "ep=[]\n",
    "for step in range(maxsteps):\n",
    "    selection=epsilonGreedy()\n",
    "    action_attempts[selection]+=1\n",
    "    #print(selection)\n",
    "    reward=0\n",
    "    if(random.uniform(0, 1)<mab[selection-1]):\n",
    "        reward=1\n",
    "        score+=1\n",
    "    Q[selection]+=((1/action_attempts[selection])*(reward-Q[selection]))\n",
    "    ep.append(score)\n",
    "print(Q)\n",
    "print(\"Score: \"+str(score))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#epsilon greedy with decay\n",
    "epsilon=1\n",
    "decayRate=.9\n",
    "print(\"\\nEpsilon Greedy with Decay\")\n",
    "Q={i:0 for i in range(1,num_bandits+1)}\n",
    "action_attempts={i:0 for i in range(1,num_bandits+1)}\n",
    "score=0\n",
    "ep_dec=[]\n",
    "for step in range(maxsteps):\n",
    "    epsilon*=decayRate\n",
    "    selection=epsilonGreedy()\n",
    "    action_attempts[selection]+=1\n",
    "    reward=0\n",
    "    if(random.uniform(0, 1)<mab[selection-1]):\n",
    "        reward=1\n",
    "        score+=1\n",
    "    Q[selection]+=((1/action_attempts[selection])*(reward-Q[selection]))\n",
    "    ep_dec.append(score)\n",
    "print(Q)\n",
    "print(\"Score: \"+str(score))\n",
    "\n",
    "\n",
    "#Thompson sampling\n",
    "alpha=[1]*num_bandits\n",
    "beta=[1]*num_bandits\n",
    "print(\"\\nThompson Sampling\")\n",
    "score=0\n",
    "ts=[]\n",
    "for step in range(maxsteps):\n",
    "    sample=[np.random.beta(alpha[x],beta[x]) for x in range(num_bandits)]\n",
    "    selection=sample.index(max(sample))\n",
    "    reward=0\n",
    "    if(random.uniform(0, 1)<mab[selection-1]):\n",
    "        reward=1\n",
    "        score+=1\n",
    "    alpha[selection]+=reward\n",
    "    beta[selection]+=(1-reward)\n",
    "    ts.append(score)\n",
    "print(\"Score: \"+str(score))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#UCB\n",
    "print(\"\\nUCB\")\n",
    "ucb=[]\n",
    "probs=[1]*num_bandits\n",
    "action_attempts={i:0 for i in range(1,num_bandits+1)}\n",
    "score=0\n",
    "np.seterr(divide = 'ignore') \n",
    "for step in range(maxsteps):\n",
    "    sample=[probs[x]+np.sqrt(np.log(step)/(1+action_attempts[x+1])) for x in range(num_bandits)]\n",
    "    selection=sample.index(max(sample))\n",
    "    action_attempts[selection+1]+=1\n",
    "    reward=0\n",
    "    if(random.uniform(0, 1)<mab[selection]):\n",
    "        reward=1\n",
    "        score+=1\n",
    "    probs[selection]+=((1/action_attempts[selection+1])*(reward-probs[selection]))\n",
    "    ucb.append(score)\n",
    "print(probs)\n",
    "print(\"Score: \"+str(score))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = np.arange(maxsteps)\n",
    "figure = plt.figure()\n",
    "tick_plot = figure.add_subplot(1, 1, 1)\n",
    "tick_plot.plot(X, ucb,  color='blue', linestyle='-',label='UCB')\n",
    "tick_plot.plot(X, rando,  color='green', linestyle='-',label='Random')\n",
    "tick_plot.plot(X, ts,  color='red', linestyle='-',label='Thompson Sampling')\n",
    "tick_plot.plot(X, ep_dec,  color='yellow', linestyle='-',label='Epsilon Greedy')\n",
    "tick_plot.plot(X, ep,  color='orange', linestyle='-',label='Epsilon')\n",
    "tick_plot.plot(X, gre,  color='purple', linestyle='-',label='Greedy')\n",
    "plt.title(\"Scores of different RL Policies\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "# References\n",
    "-  http://colah.github.io/posts/2015-08-Understanding-LSTMs/ (Chris Olah)\n",
    "-  https://github.com/nicodjimenez/lstm/blob/master/lstm.py (Nick Jimenez)\n",
    "-  https://opendata-renewables.engie.com/explore/dataset/la-haute-borne-data-2013-2016/table/ (Wind Direction Dataset, Engie)\n",
    "-  https://mattgorb.github.io/wind_multivariatelstm,https://mattgorb.github.io/wind (Matt Gorbett)\n",
    "-  https://www.quora.com/What-is-Stationary-series-and-non-Stationary-series\n",
    "-  https://towardsdatascience.com/how-not-to-use-machine-learning-for-time-series-forecasting-avoiding-the-pitfalls-19f9d7adf424\n",
    "-  https://github.com/keras-team/keras/issues/9385\n",
    "-  https://medium.com/cindicator/backtesting-time-series-models-weekend-of-a-data-scientist-92079cc2c540\n",
    "-  https://stats.stackexchange.com/questions/323867/upper-confidence-bound-in-machine-learning\n",
    "-  https://www.quora.com/What-is-Thompson-sampling-in-laymans-terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wordcount'></a>\n",
    "# Project Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file Final-Matt Gorbett.ipynb is 3143\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from IPython.nbformat import current\n",
    "import glob\n",
    "nbfile = glob.glob('Final-Matt Gorbett.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
